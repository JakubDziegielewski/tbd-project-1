16:59:06  Running with dbt=1.7.13
16:59:06  Installing dbt-labs/dbt_utils
16:59:06  Installed from version 1.1.1
16:59:06  Updated version available: 1.2.0
16:59:06  
16:59:06  Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
16:59:10  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd86d79670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd85335340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd85335520>]}
16:59:10  Running with dbt=1.7.13
16:59:10  running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/jupyter/git/tbd-tpc-di/logs', 'debug': 'True', 'profiles_dir': '/home/jupyter/git/tbd-tpc-di', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run -d', 'send_anonymous_usage_stats': 'True'}
16:59:11  Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd864fe1c0>]}
16:59:11  Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7f3ded90>]}
16:59:11  Registered adapter: spark=1.7.1
16:59:11  checksum: 6c24e2ee36a42cedd7e5de2827d214776eb7a8e8f425f6d8acb06c31add0afef, vars: {}, profile: , target: , version: 1.7.13
16:59:11  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
16:59:11  Partial parsing enabled, no changes found, skipping parsing
16:59:11  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7f05bd30>]}
16:59:11  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7f2a2040>]}
16:59:11  Found 46 models, 8 tests, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models
16:59:11  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7f2870a0>]}
16:59:11  
16:59:11  Acquiring new spark connection 'master'
16:59:11  Acquiring new spark connection 'list_schemas'
16:59:11  Using spark connection "list_schemas"
16:59:11  On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
16:59:11  Opening a new connection, currently in state init
:: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
com.databricks#spark-xml_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-a0f6c70f-db54-4fe9-a069-c67b83bb4143;1.0
	confs: [default]
	found com.databricks#spark-xml_2.12;0.17.0 in central
	found commons-io#commons-io;2.11.0 in central
	found org.glassfish.jaxb#txw2;3.0.2 in central
	found org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central
	found org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central
:: resolution report :: resolve 510ms :: artifacts dl 28ms
	:: modules in use:
	com.databricks#spark-xml_2.12;0.17.0 from central in [default]
	commons-io#commons-io;2.11.0 from central in [default]
	org.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]
	org.glassfish.jaxb#txw2;3.0.2 from central in [default]
	org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-a0f6c70f-db54-4fe9-a069-c67b83bb4143
	confs: [default]
	0 artifacts copied, 5 already retrieved (0kB/12ms)
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
24/06/20 16:59:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/20 16:59:19 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
24/06/20 16:59:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/06/20 16:59:27 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.
24/06/20 16:59:27 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.
24/06/20 16:59:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.
24/06/20 16:59:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.
24/06/20 16:59:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.
24/06/20 16:59:53 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic
16:59:54  SQL status: OK in 43.0 seconds
16:59:54  On list_schemas: Close
16:59:54  Using spark connection "list_schemas"
16:59:54  On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
16:59:54  Opening a new connection, currently in state closed
16:59:54  SQL status: OK in 0.0 seconds
16:59:54  On list_schemas: Close
16:59:54  Using spark connection "list_schemas"
16:59:54  On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
16:59:54  Opening a new connection, currently in state closed
16:59:54  SQL status: OK in 0.0 seconds
16:59:54  On list_schemas: Close
16:59:54  Acquiring new spark connection 'list_None_demo_bronze'
16:59:55  Spark adapter: NotImplemented: add_begin_query
16:59:55  Using spark connection "list_None_demo_bronze"
16:59:55  On list_None_demo_bronze: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "connection_name": "list_None_demo_bronze"} */
show table extended in demo_bronze like '*'
  
16:59:55  Opening a new connection, currently in state init
16:59:55  SQL status: OK in 0.0 seconds
16:59:55  On list_None_demo_bronze: ROLLBACK
16:59:55  Spark adapter: NotImplemented: rollback
16:59:55  On list_None_demo_bronze: Close
16:59:55  Re-using an available connection from the pool (formerly list_None_demo_bronze, now list_None_demo_silver)
16:59:55  Spark adapter: NotImplemented: add_begin_query
16:59:55  Using spark connection "list_None_demo_silver"
16:59:55  On list_None_demo_silver: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "connection_name": "list_None_demo_silver"} */
show table extended in demo_silver like '*'
  
16:59:55  Opening a new connection, currently in state closed
16:59:56  SQL status: OK in 1.0 seconds
16:59:56  On list_None_demo_silver: ROLLBACK
16:59:56  Spark adapter: NotImplemented: rollback
16:59:56  On list_None_demo_silver: Close
16:59:56  Re-using an available connection from the pool (formerly list_None_demo_silver, now list_None_demo_gold)
16:59:56  Spark adapter: NotImplemented: add_begin_query
16:59:56  Using spark connection "list_None_demo_gold"
16:59:56  On list_None_demo_gold: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "connection_name": "list_None_demo_gold"} */
show table extended in demo_gold like '*'
  
16:59:56  Opening a new connection, currently in state closed
16:59:56  SQL status: OK in 0.0 seconds
16:59:56  On list_None_demo_gold: ROLLBACK
16:59:56  Spark adapter: NotImplemented: rollback
16:59:56  On list_None_demo_gold: Close
16:59:56  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7f3fb9a0>]}
16:59:56  Spark adapter: NotImplemented: add_begin_query
16:59:56  Spark adapter: NotImplemented: commit
16:59:56  Concurrency: 1 threads (target='dev')
16:59:56  
16:59:56  Began running node model.tbd_tpcdi.brokerage_cash_transaction
16:59:56  1 of 45 START sql table model demo_bronze.brokerage_cash_transaction ........... [RUN]
16:59:56  Re-using an available connection from the pool (formerly list_None_demo_gold, now model.tbd_tpcdi.brokerage_cash_transaction)
16:59:56  Began compiling node model.tbd_tpcdi.brokerage_cash_transaction
16:59:56  Writing injected SQL for node "model.tbd_tpcdi.brokerage_cash_transaction"
16:59:56  Timing info for model.tbd_tpcdi.brokerage_cash_transaction (compile): 16:59:56.771404 => 16:59:56.797201
16:59:56  Began executing node model.tbd_tpcdi.brokerage_cash_transaction
16:59:56  Using spark connection "model.tbd_tpcdi.brokerage_cash_transaction"
16:59:56  On model.tbd_tpcdi.brokerage_cash_transaction: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction"} */
drop table if exists demo_bronze.brokerage_cash_transaction
16:59:56  Opening a new connection, currently in state closed
16:59:56  SQL status: OK in 0.0 seconds
16:59:57  Writing runtime sql for node "model.tbd_tpcdi.brokerage_cash_transaction"
16:59:57  Spark adapter: NotImplemented: add_begin_query
16:59:57  Using spark connection "model.tbd_tpcdi.brokerage_cash_transaction"
16:59:57  On model.tbd_tpcdi.brokerage_cash_transaction: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction"} */

  
    
        create table demo_bronze.brokerage_cash_transaction
      
      
      
      
      
      
      
      

      as
      select *
from digen.cash_transaction
  
24/06/20 16:59:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
24/06/20 16:59:58 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
17:00:25  SQL status: OK in 28.0 seconds
17:00:25  Timing info for model.tbd_tpcdi.brokerage_cash_transaction (execute): 16:59:56.800727 => 17:00:25.381726
17:00:25  On model.tbd_tpcdi.brokerage_cash_transaction: ROLLBACK
17:00:25  Spark adapter: NotImplemented: rollback
17:00:25  On model.tbd_tpcdi.brokerage_cash_transaction: Close
17:00:25  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d61a820>]}
17:00:25  1 of 45 OK created sql table model demo_bronze.brokerage_cash_transaction ...... [OK in 28.62s]
17:00:25  Finished running node model.tbd_tpcdi.brokerage_cash_transaction
17:00:25  Began running node model.tbd_tpcdi.brokerage_cash_transaction-checkpoint
17:00:25  2 of 45 START sql table model demo_bronze.brokerage_cash_transaction-checkpoint  [RUN]
17:00:25  Re-using an available connection from the pool (formerly model.tbd_tpcdi.brokerage_cash_transaction, now model.tbd_tpcdi.brokerage_cash_transaction-checkpoint)
17:00:25  Began compiling node model.tbd_tpcdi.brokerage_cash_transaction-checkpoint
17:00:25  Writing injected SQL for node "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"
17:00:25  Timing info for model.tbd_tpcdi.brokerage_cash_transaction-checkpoint (compile): 17:00:25.417230 => 17:00:25.439944
17:00:25  Began executing node model.tbd_tpcdi.brokerage_cash_transaction-checkpoint
17:00:25  Using spark connection "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"
17:00:25  On model.tbd_tpcdi.brokerage_cash_transaction-checkpoint: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"} */
drop table if exists demo_bronze.brokerage_cash_transaction-checkpoint
17:00:25  Opening a new connection, currently in state closed
17:00:25  Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"} */
drop table if exists demo_bronze.brokerage_cash_transaction-checkpoint
17:00:25  Spark adapter: 
Possibly unquoted identifier brokerage_cash_transaction-checkpoint detected. Please consider quoting it with back-quotes as `brokerage_cash_transaction-checkpoint`(line 2, pos 59)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"} */
drop table if exists demo_bronze.brokerage_cash_transaction-checkpoint
-----------------------------------------------------------^^^

17:00:25  Spark adapter: Error while running:
macro drop_relation
17:00:25  Spark adapter: 
Possibly unquoted identifier brokerage_cash_transaction-checkpoint detected. Please consider quoting it with back-quotes as `brokerage_cash_transaction-checkpoint`(line 2, pos 59)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"} */
drop table if exists demo_bronze.brokerage_cash_transaction-checkpoint
-----------------------------------------------------------^^^

17:00:25  Timing info for model.tbd_tpcdi.brokerage_cash_transaction-checkpoint (execute): 17:00:25.449361 => 17:00:25.566988
17:00:25  On model.tbd_tpcdi.brokerage_cash_transaction-checkpoint: Close
17:00:25  Unhandled error while executing 
Possibly unquoted identifier brokerage_cash_transaction-checkpoint detected. Please consider quoting it with back-quotes as `brokerage_cash_transaction-checkpoint`(line 2, pos 59)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"} */
drop table if exists demo_bronze.brokerage_cash_transaction-checkpoint
-----------------------------------------------------------^^^
17:00:25  Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 380, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 326, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 427, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/run.py", line 292, in execute
    result = MacroGenerator(
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 99, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/sql/impl.py", line 142, in drop_relation
    self.execute_macro(DROP_RELATION_MACRO_NAME, kwargs={"relation": relation})
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 1112, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 20, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 29, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 310, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/sql/connections.py", line 138, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/spark/session.py", line 227, in execute
    self._cursor.execute(sql)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/spark/session.py", line 124, in execute
    self._df = spark_session.sql(sql)
  File "/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py", line 1034, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self)
  File "/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py", line 196, in deco
    raise converted from None
pyspark.sql.utils.ParseException: 
Possibly unquoted identifier brokerage_cash_transaction-checkpoint detected. Please consider quoting it with back-quotes as `brokerage_cash_transaction-checkpoint`(line 2, pos 59)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"} */
drop table if exists demo_bronze.brokerage_cash_transaction-checkpoint
-----------------------------------------------------------^^^


17:00:25  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d5e3c10>]}
17:00:25  2 of 45 ERROR creating sql table model demo_bronze.brokerage_cash_transaction-checkpoint  [ERROR in 0.19s]
17:00:25  Finished running node model.tbd_tpcdi.brokerage_cash_transaction-checkpoint
17:00:25  Began running node model.tbd_tpcdi.brokerage_daily_market
17:00:25  3 of 45 START sql table model demo_bronze.brokerage_daily_market ............... [RUN]
17:00:25  Re-using an available connection from the pool (formerly model.tbd_tpcdi.brokerage_cash_transaction-checkpoint, now model.tbd_tpcdi.brokerage_daily_market)
17:00:25  Began compiling node model.tbd_tpcdi.brokerage_daily_market
17:00:25  Writing injected SQL for node "model.tbd_tpcdi.brokerage_daily_market"
17:00:25  Timing info for model.tbd_tpcdi.brokerage_daily_market (compile): 17:00:25.611028 => 17:00:25.621280
17:00:25  Began executing node model.tbd_tpcdi.brokerage_daily_market
17:00:25  Using spark connection "model.tbd_tpcdi.brokerage_daily_market"
17:00:25  On model.tbd_tpcdi.brokerage_daily_market: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_daily_market"} */
drop table if exists demo_bronze.brokerage_daily_market
17:00:25  Opening a new connection, currently in state closed
17:00:25  SQL status: OK in 0.0 seconds
17:00:25  Writing runtime sql for node "model.tbd_tpcdi.brokerage_daily_market"
17:00:25  Spark adapter: NotImplemented: add_begin_query
17:00:25  Using spark connection "model.tbd_tpcdi.brokerage_daily_market"
17:00:25  On model.tbd_tpcdi.brokerage_daily_market: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_daily_market"} */

  
    
        create table demo_bronze.brokerage_daily_market
      
      
      
      
      
      
      
      

      as
      select
    *
from digen.daily_market
  
24/06/20 17:00:25 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:01:18  SQL status: OK in 52.0 seconds
17:01:18  Timing info for model.tbd_tpcdi.brokerage_daily_market (execute): 17:00:25.624236 => 17:01:18.211007
17:01:18  On model.tbd_tpcdi.brokerage_daily_market: ROLLBACK
17:01:18  Spark adapter: NotImplemented: rollback
17:01:18  On model.tbd_tpcdi.brokerage_daily_market: Close
17:01:18  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c547fd0>]}
17:01:18  3 of 45 OK created sql table model demo_bronze.brokerage_daily_market .......... [OK in 52.61s]
17:01:18  Finished running node model.tbd_tpcdi.brokerage_daily_market
17:01:18  Began running node model.tbd_tpcdi.brokerage_holding_history
17:01:18  4 of 45 START sql table model demo_bronze.brokerage_holding_history ............ [RUN]
17:01:18  Re-using an available connection from the pool (formerly model.tbd_tpcdi.brokerage_daily_market, now model.tbd_tpcdi.brokerage_holding_history)
17:01:18  Began compiling node model.tbd_tpcdi.brokerage_holding_history
17:01:18  Writing injected SQL for node "model.tbd_tpcdi.brokerage_holding_history"
17:01:18  Timing info for model.tbd_tpcdi.brokerage_holding_history (compile): 17:01:18.230172 => 17:01:18.241387
17:01:18  Began executing node model.tbd_tpcdi.brokerage_holding_history
17:01:18  Using spark connection "model.tbd_tpcdi.brokerage_holding_history"
17:01:18  On model.tbd_tpcdi.brokerage_holding_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_holding_history"} */
drop table if exists demo_bronze.brokerage_holding_history
17:01:18  Opening a new connection, currently in state closed
17:01:18  SQL status: OK in 0.0 seconds
17:01:18  Writing runtime sql for node "model.tbd_tpcdi.brokerage_holding_history"
17:01:18  Spark adapter: NotImplemented: add_begin_query
17:01:18  Using spark connection "model.tbd_tpcdi.brokerage_holding_history"
17:01:18  On model.tbd_tpcdi.brokerage_holding_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_holding_history"} */

  
    
        create table demo_bronze.brokerage_holding_history
      
      
      
      
      
      
      
      

      as
      select *
from digen.holding_history
  
24/06/20 17:01:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:01:25  SQL status: OK in 7.0 seconds
17:01:25  Timing info for model.tbd_tpcdi.brokerage_holding_history (execute): 17:01:18.244446 => 17:01:25.247847
17:01:25  On model.tbd_tpcdi.brokerage_holding_history: ROLLBACK
17:01:25  Spark adapter: NotImplemented: rollback
17:01:25  On model.tbd_tpcdi.brokerage_holding_history: Close
17:01:25  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d5e3c70>]}
17:01:25  4 of 45 OK created sql table model demo_bronze.brokerage_holding_history ....... [OK in 7.04s]
17:01:25  Finished running node model.tbd_tpcdi.brokerage_holding_history
17:01:25  Began running node model.tbd_tpcdi.brokerage_trade
17:01:25  5 of 45 START sql table model demo_bronze.brokerage_trade ...................... [RUN]
17:01:25  Re-using an available connection from the pool (formerly model.tbd_tpcdi.brokerage_holding_history, now model.tbd_tpcdi.brokerage_trade)
17:01:25  Began compiling node model.tbd_tpcdi.brokerage_trade
17:01:25  Writing injected SQL for node "model.tbd_tpcdi.brokerage_trade"
17:01:25  Timing info for model.tbd_tpcdi.brokerage_trade (compile): 17:01:25.289899 => 17:01:25.305597
17:01:25  Began executing node model.tbd_tpcdi.brokerage_trade
17:01:25  Using spark connection "model.tbd_tpcdi.brokerage_trade"
17:01:25  On model.tbd_tpcdi.brokerage_trade: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_trade"} */
drop table if exists demo_bronze.brokerage_trade
17:01:25  Opening a new connection, currently in state closed
17:01:25  SQL status: OK in 0.0 seconds
17:01:25  Writing runtime sql for node "model.tbd_tpcdi.brokerage_trade"
17:01:25  Spark adapter: NotImplemented: add_begin_query
17:01:25  Using spark connection "model.tbd_tpcdi.brokerage_trade"
17:01:25  On model.tbd_tpcdi.brokerage_trade: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_trade"} */

  
    
        create table demo_bronze.brokerage_trade
      
      
      
      
      
      
      
      

      as
      select *
from digen.trade
  
24/06/20 17:01:25 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:01:59  SQL status: OK in 34.0 seconds
17:01:59  Timing info for model.tbd_tpcdi.brokerage_trade (execute): 17:01:25.309141 => 17:01:59.535849
17:01:59  On model.tbd_tpcdi.brokerage_trade: ROLLBACK
17:01:59  Spark adapter: NotImplemented: rollback
17:01:59  On model.tbd_tpcdi.brokerage_trade: Close
17:01:59  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d5e3820>]}
17:01:59  5 of 45 OK created sql table model demo_bronze.brokerage_trade ................. [OK in 34.27s]
17:01:59  Finished running node model.tbd_tpcdi.brokerage_trade
17:01:59  Began running node model.tbd_tpcdi.brokerage_trade_history
17:01:59  6 of 45 START sql table model demo_bronze.brokerage_trade_history .............. [RUN]
17:01:59  Re-using an available connection from the pool (formerly model.tbd_tpcdi.brokerage_trade, now model.tbd_tpcdi.brokerage_trade_history)
17:01:59  Began compiling node model.tbd_tpcdi.brokerage_trade_history
17:01:59  Writing injected SQL for node "model.tbd_tpcdi.brokerage_trade_history"
17:01:59  Timing info for model.tbd_tpcdi.brokerage_trade_history (compile): 17:01:59.584338 => 17:01:59.594914
17:01:59  Began executing node model.tbd_tpcdi.brokerage_trade_history
17:01:59  Using spark connection "model.tbd_tpcdi.brokerage_trade_history"
17:01:59  On model.tbd_tpcdi.brokerage_trade_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_trade_history"} */
drop table if exists demo_bronze.brokerage_trade_history
17:01:59  Opening a new connection, currently in state closed
17:01:59  SQL status: OK in 0.0 seconds
17:01:59  Writing runtime sql for node "model.tbd_tpcdi.brokerage_trade_history"
17:01:59  Spark adapter: NotImplemented: add_begin_query
17:01:59  Using spark connection "model.tbd_tpcdi.brokerage_trade_history"
17:01:59  On model.tbd_tpcdi.brokerage_trade_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_trade_history"} */

  
    
        create table demo_bronze.brokerage_trade_history
      
      
      
      
      
      
      
      

      as
      select *
from digen.trade_history
  
24/06/20 17:01:59 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:02:23  SQL status: OK in 23.0 seconds
17:02:23  Timing info for model.tbd_tpcdi.brokerage_trade_history (execute): 17:01:59.597467 => 17:02:23.173734
17:02:23  On model.tbd_tpcdi.brokerage_trade_history: ROLLBACK
17:02:23  Spark adapter: NotImplemented: rollback
17:02:23  On model.tbd_tpcdi.brokerage_trade_history: Close
17:02:23  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d61a1f0>]}
17:02:23  6 of 45 OK created sql table model demo_bronze.brokerage_trade_history ......... [OK in 23.60s]
17:02:23  Finished running node model.tbd_tpcdi.brokerage_trade_history
17:02:23  Began running node model.tbd_tpcdi.brokerage_watch_history
17:02:23  7 of 45 START sql table model demo_bronze.brokerage_watch_history .............. [RUN]
17:02:23  Re-using an available connection from the pool (formerly model.tbd_tpcdi.brokerage_trade_history, now model.tbd_tpcdi.brokerage_watch_history)
17:02:23  Began compiling node model.tbd_tpcdi.brokerage_watch_history
17:02:23  Writing injected SQL for node "model.tbd_tpcdi.brokerage_watch_history"
17:02:23  Timing info for model.tbd_tpcdi.brokerage_watch_history (compile): 17:02:23.189533 => 17:02:23.199330
17:02:23  Began executing node model.tbd_tpcdi.brokerage_watch_history
17:02:23  Using spark connection "model.tbd_tpcdi.brokerage_watch_history"
17:02:23  On model.tbd_tpcdi.brokerage_watch_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_watch_history"} */
drop table if exists demo_bronze.brokerage_watch_history
17:02:23  Opening a new connection, currently in state closed
17:02:23  SQL status: OK in 0.0 seconds
17:02:23  Writing runtime sql for node "model.tbd_tpcdi.brokerage_watch_history"
17:02:23  Spark adapter: NotImplemented: add_begin_query
17:02:23  Using spark connection "model.tbd_tpcdi.brokerage_watch_history"
17:02:23  On model.tbd_tpcdi.brokerage_watch_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_watch_history"} */

  
    
        create table demo_bronze.brokerage_watch_history
      
      
      
      
      
      
      
      

      as
      select
    *
from digen.watch_history
  
24/06/20 17:02:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:02:59  SQL status: OK in 37.0 seconds
17:02:59  Timing info for model.tbd_tpcdi.brokerage_watch_history (execute): 17:02:23.202050 => 17:02:59.858906
17:02:59  On model.tbd_tpcdi.brokerage_watch_history: ROLLBACK
17:02:59  Spark adapter: NotImplemented: rollback
17:02:59  On model.tbd_tpcdi.brokerage_watch_history: Close
17:02:59  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c52a760>]}
17:02:59  7 of 45 OK created sql table model demo_bronze.brokerage_watch_history ......... [OK in 36.68s]
17:02:59  Finished running node model.tbd_tpcdi.brokerage_watch_history
17:02:59  Began running node model.tbd_tpcdi.crm_customer_mgmt
17:02:59  8 of 45 START sql table model demo_bronze.crm_customer_mgmt .................... [RUN]
17:02:59  Re-using an available connection from the pool (formerly model.tbd_tpcdi.brokerage_watch_history, now model.tbd_tpcdi.crm_customer_mgmt)
17:02:59  Began compiling node model.tbd_tpcdi.crm_customer_mgmt
17:02:59  Writing injected SQL for node "model.tbd_tpcdi.crm_customer_mgmt"
17:02:59  Timing info for model.tbd_tpcdi.crm_customer_mgmt (compile): 17:02:59.874284 => 17:02:59.886062
17:02:59  Began executing node model.tbd_tpcdi.crm_customer_mgmt
17:02:59  Using spark connection "model.tbd_tpcdi.crm_customer_mgmt"
17:02:59  On model.tbd_tpcdi.crm_customer_mgmt: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.crm_customer_mgmt"} */
drop table if exists demo_bronze.crm_customer_mgmt
17:02:59  Opening a new connection, currently in state closed
17:02:59  SQL status: OK in 0.0 seconds
17:02:59  Writing runtime sql for node "model.tbd_tpcdi.crm_customer_mgmt"
17:02:59  Spark adapter: NotImplemented: add_begin_query
17:02:59  Using spark connection "model.tbd_tpcdi.crm_customer_mgmt"
17:02:59  On model.tbd_tpcdi.crm_customer_mgmt: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.crm_customer_mgmt"} */

  
    
        create table demo_bronze.crm_customer_mgmt
      
      
      
      
      
      
      
      

      as
      select *
from digen.customer_mgmt
  
24/06/20 17:03:00 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
24/06/20 17:03:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
17:03:06  SQL status: OK in 6.0 seconds
17:03:06  Timing info for model.tbd_tpcdi.crm_customer_mgmt (execute): 17:02:59.888689 => 17:03:06.176457
17:03:06  On model.tbd_tpcdi.crm_customer_mgmt: ROLLBACK
17:03:06  Spark adapter: NotImplemented: rollback
17:03:06  On model.tbd_tpcdi.crm_customer_mgmt: Close
17:03:06  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c556c70>]}
17:03:06  8 of 45 OK created sql table model demo_bronze.crm_customer_mgmt ............... [OK in 6.31s]
17:03:06  Finished running node model.tbd_tpcdi.crm_customer_mgmt
17:03:06  Began running node model.tbd_tpcdi.finwire_company
17:03:06  9 of 45 START sql table model demo_bronze.finwire_company ...................... [RUN]
17:03:06  Re-using an available connection from the pool (formerly model.tbd_tpcdi.crm_customer_mgmt, now model.tbd_tpcdi.finwire_company)
17:03:06  Began compiling node model.tbd_tpcdi.finwire_company
17:03:06  Writing injected SQL for node "model.tbd_tpcdi.finwire_company"
17:03:06  Timing info for model.tbd_tpcdi.finwire_company (compile): 17:03:06.202004 => 17:03:06.213370
17:03:06  Began executing node model.tbd_tpcdi.finwire_company
17:03:06  Using spark connection "model.tbd_tpcdi.finwire_company"
17:03:06  On model.tbd_tpcdi.finwire_company: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.finwire_company"} */
drop table if exists demo_bronze.finwire_company
17:03:06  Opening a new connection, currently in state closed
17:03:06  SQL status: OK in 0.0 seconds
17:03:06  Writing runtime sql for node "model.tbd_tpcdi.finwire_company"
17:03:06  Spark adapter: NotImplemented: add_begin_query
17:03:06  Using spark connection "model.tbd_tpcdi.finwire_company"
17:03:06  On model.tbd_tpcdi.finwire_company: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.finwire_company"} */

  
    
        create table demo_bronze.finwire_company
      
      
      
      
      
      
      
      

      as
      select
    *
from digen.cmp
  
24/06/20 17:03:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:08  SQL status: OK in 2.0 seconds
17:03:08  Timing info for model.tbd_tpcdi.finwire_company (execute): 17:03:06.218899 => 17:03:08.292640
17:03:08  On model.tbd_tpcdi.finwire_company: ROLLBACK
17:03:08  Spark adapter: NotImplemented: rollback
17:03:08  On model.tbd_tpcdi.finwire_company: Close
17:03:08  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4f7d00>]}
17:03:08  9 of 45 OK created sql table model demo_bronze.finwire_company ................. [OK in 2.10s]
17:03:08  Finished running node model.tbd_tpcdi.finwire_company
17:03:08  Began running node model.tbd_tpcdi.finwire_financial
17:03:08  10 of 45 START sql table model demo_bronze.finwire_financial ................... [RUN]
17:03:08  Re-using an available connection from the pool (formerly model.tbd_tpcdi.finwire_company, now model.tbd_tpcdi.finwire_financial)
17:03:08  Began compiling node model.tbd_tpcdi.finwire_financial
17:03:08  Writing injected SQL for node "model.tbd_tpcdi.finwire_financial"
17:03:08  Timing info for model.tbd_tpcdi.finwire_financial (compile): 17:03:08.308581 => 17:03:08.321766
17:03:08  Began executing node model.tbd_tpcdi.finwire_financial
17:03:08  Using spark connection "model.tbd_tpcdi.finwire_financial"
17:03:08  On model.tbd_tpcdi.finwire_financial: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.finwire_financial"} */
drop table if exists demo_bronze.finwire_financial
17:03:08  Opening a new connection, currently in state closed
17:03:08  SQL status: OK in 0.0 seconds
17:03:08  Writing runtime sql for node "model.tbd_tpcdi.finwire_financial"
17:03:08  Spark adapter: NotImplemented: add_begin_query
17:03:08  Using spark connection "model.tbd_tpcdi.finwire_financial"
17:03:08  On model.tbd_tpcdi.finwire_financial: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.finwire_financial"} */

  
    
        create table demo_bronze.finwire_financial
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select 
        *,
        try_to_number(co_name_or_cik, '9999999999') as try_cik
    from digen.fin
)
select 
    pts,
    to_number(year, '9999') as year,
    to_number(quarter, '99') as quarter,
    to_date(quarter_start_date,'yyyymmdd') as quarter_start_date,
    to_date(posting_date,'yyyymmdd') as posting_date,
    cast(revenue as float) as revenue,
    cast(earnings as float) as earnings,
    cast(eps as float) as eps,
    cast(diluted_eps as float) as diluted_eps,
    cast(margin as float) as margin,
    cast(inventory as float) as inventory,
    cast(assets as float) as assets,
    cast(liabilities as float) as liabilities,
    to_number(sh_out, '9999999999') as sh_out,
    to_number(diluted_sh_out, '9999999999') as diluted_sh_out,
    try_cik cik,
    case when try_cik is null then co_name_or_cik else null end company_name
from s1
  
24/06/20 17:03:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:43  SQL status: OK in 35.0 seconds
17:03:43  Timing info for model.tbd_tpcdi.finwire_financial (execute): 17:03:08.324400 => 17:03:43.896418
17:03:43  On model.tbd_tpcdi.finwire_financial: ROLLBACK
17:03:43  Spark adapter: NotImplemented: rollback
17:03:43  On model.tbd_tpcdi.finwire_financial: Close
17:03:43  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d631f70>]}
17:03:43  10 of 45 OK created sql table model demo_bronze.finwire_financial .............. [OK in 35.60s]
17:03:43  Finished running node model.tbd_tpcdi.finwire_financial
17:03:43  Began running node model.tbd_tpcdi.finwire_security
17:03:43  11 of 45 START sql table model demo_bronze.finwire_security .................... [RUN]
17:03:43  Re-using an available connection from the pool (formerly model.tbd_tpcdi.finwire_financial, now model.tbd_tpcdi.finwire_security)
17:03:43  Began compiling node model.tbd_tpcdi.finwire_security
17:03:43  Writing injected SQL for node "model.tbd_tpcdi.finwire_security"
17:03:43  Timing info for model.tbd_tpcdi.finwire_security (compile): 17:03:43.913967 => 17:03:43.924377
17:03:43  Began executing node model.tbd_tpcdi.finwire_security
17:03:43  Using spark connection "model.tbd_tpcdi.finwire_security"
17:03:43  On model.tbd_tpcdi.finwire_security: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.finwire_security"} */
drop table if exists demo_bronze.finwire_security
17:03:43  Opening a new connection, currently in state closed
17:03:43  SQL status: OK in 0.0 seconds
17:03:43  Writing runtime sql for node "model.tbd_tpcdi.finwire_security"
17:03:43  Spark adapter: NotImplemented: add_begin_query
17:03:43  Using spark connection "model.tbd_tpcdi.finwire_security"
17:03:43  On model.tbd_tpcdi.finwire_security: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.finwire_security"} */

  
    
        create table demo_bronze.finwire_security
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select *,
    try_to_number(co_name_or_cik, '9999999999') as try_cik
    from digen.sec
)
select  
    pts,
    symbol,
    issue_type,
    status,
    name,
    ex_id,
    to_number(sh_out, '9999999999') as sh_out,
    to_date(first_trade_date,'yyyymmdd') as first_trade_date,
    to_date(first_exchange_date,'yyyymmdd') as first_exchange_date,
    cast(dividend as float) as dividend,
    try_cik cik,
    case when try_cik is null then co_name_or_cik else null end company_name
from s1
  
24/06/20 17:03:44 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:46  SQL status: OK in 2.0 seconds
17:03:46  Timing info for model.tbd_tpcdi.finwire_security (execute): 17:03:43.927021 => 17:03:46.184513
17:03:46  On model.tbd_tpcdi.finwire_security: ROLLBACK
17:03:46  Spark adapter: NotImplemented: rollback
17:03:46  On model.tbd_tpcdi.finwire_security: Close
17:03:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4f7d00>]}
17:03:46  11 of 45 OK created sql table model demo_bronze.finwire_security ............... [OK in 2.28s]
17:03:46  Finished running node model.tbd_tpcdi.finwire_security
17:03:46  Began running node model.tbd_tpcdi.hr_employee
17:03:46  12 of 45 START sql table model demo_bronze.hr_employee ......................... [RUN]
17:03:46  Re-using an available connection from the pool (formerly model.tbd_tpcdi.finwire_security, now model.tbd_tpcdi.hr_employee)
17:03:46  Began compiling node model.tbd_tpcdi.hr_employee
17:03:46  Writing injected SQL for node "model.tbd_tpcdi.hr_employee"
17:03:46  Timing info for model.tbd_tpcdi.hr_employee (compile): 17:03:46.214607 => 17:03:46.227280
17:03:46  Began executing node model.tbd_tpcdi.hr_employee
17:03:46  Using spark connection "model.tbd_tpcdi.hr_employee"
17:03:46  On model.tbd_tpcdi.hr_employee: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.hr_employee"} */
drop table if exists demo_bronze.hr_employee
17:03:46  Opening a new connection, currently in state closed
17:03:46  SQL status: OK in 0.0 seconds
17:03:46  Writing runtime sql for node "model.tbd_tpcdi.hr_employee"
17:03:46  Spark adapter: NotImplemented: add_begin_query
17:03:46  Using spark connection "model.tbd_tpcdi.hr_employee"
17:03:46  On model.tbd_tpcdi.hr_employee: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.hr_employee"} */

  
    
        create table demo_bronze.hr_employee
      
      
      
      
      
      
      
      

      as
      select *
from digen.hr
  
24/06/20 17:03:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:48  SQL status: OK in 2.0 seconds
17:03:48  Timing info for model.tbd_tpcdi.hr_employee (execute): 17:03:46.230935 => 17:03:48.277138
17:03:48  On model.tbd_tpcdi.hr_employee: ROLLBACK
17:03:48  Spark adapter: NotImplemented: rollback
17:03:48  On model.tbd_tpcdi.hr_employee: Close
17:03:48  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4f4f40>]}
17:03:48  12 of 45 OK created sql table model demo_bronze.hr_employee .................... [OK in 2.08s]
17:03:48  Finished running node model.tbd_tpcdi.hr_employee
17:03:48  Began running node model.tbd_tpcdi.reference_date
17:03:48  13 of 45 START sql table model demo_bronze.reference_date ...................... [RUN]
17:03:48  Re-using an available connection from the pool (formerly model.tbd_tpcdi.hr_employee, now model.tbd_tpcdi.reference_date)
17:03:48  Began compiling node model.tbd_tpcdi.reference_date
17:03:48  Writing injected SQL for node "model.tbd_tpcdi.reference_date"
17:03:48  Timing info for model.tbd_tpcdi.reference_date (compile): 17:03:48.307133 => 17:03:48.318932
17:03:48  Began executing node model.tbd_tpcdi.reference_date
17:03:48  Using spark connection "model.tbd_tpcdi.reference_date"
17:03:48  On model.tbd_tpcdi.reference_date: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_date"} */
drop table if exists demo_bronze.reference_date
17:03:48  Opening a new connection, currently in state closed
17:03:48  SQL status: OK in 0.0 seconds
17:03:48  Writing runtime sql for node "model.tbd_tpcdi.reference_date"
17:03:48  Spark adapter: NotImplemented: add_begin_query
17:03:48  Using spark connection "model.tbd_tpcdi.reference_date"
17:03:48  On model.tbd_tpcdi.reference_date: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_date"} */

  
    
        create table demo_bronze.reference_date
      
      
      
      
      
      
      
      

      as
      select
    DATE_VALUE SK_DATE_ID,
	DATE_VALUE,
	DATE_DESC,
	CALENDAR_YEAR_ID,
	CALENDAR_YEAR_DESC,
	CALENDAR_QTR_ID,
	CALENDAR_QTR_DESC,
	CALENDAR_MONTH_ID,
	CALENDAR_MONTH_DESC,
	CALENDAR_WEEK_ID,
	CALENDAR_WEEK_DESC,
	DAY_OF_WEEK_NUM,
	DAY_OF_WEEK_DESC,
	FISCAL_YEAR_ID,
	FISCAL_YEAR_DESC,
	FISCAL_QTR_ID,
	FISCAL_QTR_DESC,
	HOLIDAY_FLAG
from digen.date
  
24/06/20 17:03:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:49  SQL status: OK in 1.0 seconds
17:03:49  Timing info for model.tbd_tpcdi.reference_date (execute): 17:03:48.324258 => 17:03:49.510004
17:03:49  On model.tbd_tpcdi.reference_date: ROLLBACK
17:03:49  Spark adapter: NotImplemented: rollback
17:03:49  On model.tbd_tpcdi.reference_date: Close
17:03:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7de836d0>]}
17:03:49  13 of 45 OK created sql table model demo_bronze.reference_date ................. [OK in 1.21s]
17:03:49  Finished running node model.tbd_tpcdi.reference_date
17:03:49  Began running node model.tbd_tpcdi.reference_industry
17:03:49  14 of 45 START sql table model demo_bronze.reference_industry .................. [RUN]
17:03:49  Re-using an available connection from the pool (formerly model.tbd_tpcdi.reference_date, now model.tbd_tpcdi.reference_industry)
17:03:49  Began compiling node model.tbd_tpcdi.reference_industry
17:03:49  Writing injected SQL for node "model.tbd_tpcdi.reference_industry"
17:03:49  Timing info for model.tbd_tpcdi.reference_industry (compile): 17:03:49.558790 => 17:03:49.585906
17:03:49  Began executing node model.tbd_tpcdi.reference_industry
17:03:49  Using spark connection "model.tbd_tpcdi.reference_industry"
17:03:49  On model.tbd_tpcdi.reference_industry: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_industry"} */
drop table if exists demo_bronze.reference_industry
17:03:49  Opening a new connection, currently in state closed
17:03:49  SQL status: OK in 0.0 seconds
17:03:49  Writing runtime sql for node "model.tbd_tpcdi.reference_industry"
17:03:49  Spark adapter: NotImplemented: add_begin_query
17:03:49  Using spark connection "model.tbd_tpcdi.reference_industry"
17:03:49  On model.tbd_tpcdi.reference_industry: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_industry"} */

  
    
        create table demo_bronze.reference_industry
      
      
      
      
      
      
      
      

      as
      select *
from digen.industry
  
24/06/20 17:03:49 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:50  SQL status: OK in 1.0 seconds
17:03:50  Timing info for model.tbd_tpcdi.reference_industry (execute): 17:03:49.594643 => 17:03:50.383051
17:03:50  On model.tbd_tpcdi.reference_industry: ROLLBACK
17:03:50  Spark adapter: NotImplemented: rollback
17:03:50  On model.tbd_tpcdi.reference_industry: Close
17:03:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d5f0f10>]}
17:03:50  14 of 45 OK created sql table model demo_bronze.reference_industry ............. [OK in 0.84s]
17:03:50  Finished running node model.tbd_tpcdi.reference_industry
17:03:50  Began running node model.tbd_tpcdi.reference_status_type
17:03:50  15 of 45 START sql table model demo_bronze.reference_status_type ............... [RUN]
17:03:50  Re-using an available connection from the pool (formerly model.tbd_tpcdi.reference_industry, now model.tbd_tpcdi.reference_status_type)
17:03:50  Began compiling node model.tbd_tpcdi.reference_status_type
17:03:50  Writing injected SQL for node "model.tbd_tpcdi.reference_status_type"
17:03:50  Timing info for model.tbd_tpcdi.reference_status_type (compile): 17:03:50.413226 => 17:03:50.426560
17:03:50  Began executing node model.tbd_tpcdi.reference_status_type
17:03:50  Using spark connection "model.tbd_tpcdi.reference_status_type"
17:03:50  On model.tbd_tpcdi.reference_status_type: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_status_type"} */
drop table if exists demo_bronze.reference_status_type
17:03:50  Opening a new connection, currently in state closed
17:03:50  SQL status: OK in 0.0 seconds
17:03:50  Writing runtime sql for node "model.tbd_tpcdi.reference_status_type"
17:03:50  Spark adapter: NotImplemented: add_begin_query
17:03:50  Using spark connection "model.tbd_tpcdi.reference_status_type"
17:03:50  On model.tbd_tpcdi.reference_status_type: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_status_type"} */

  
    
        create table demo_bronze.reference_status_type
      
      
      
      
      
      
      
      

      as
      select *
from digen.status_type
  
24/06/20 17:03:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:51  SQL status: OK in 1.0 seconds
17:03:51  Timing info for model.tbd_tpcdi.reference_status_type (execute): 17:03:50.429226 => 17:03:51.232419
17:03:51  On model.tbd_tpcdi.reference_status_type: ROLLBACK
17:03:51  Spark adapter: NotImplemented: rollback
17:03:51  On model.tbd_tpcdi.reference_status_type: Close
17:03:51  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4f7d00>]}
17:03:51  15 of 45 OK created sql table model demo_bronze.reference_status_type .......... [OK in 0.84s]
17:03:51  Finished running node model.tbd_tpcdi.reference_status_type
17:03:51  Began running node model.tbd_tpcdi.reference_tax_rate
17:03:51  16 of 45 START sql table model demo_bronze.reference_tax_rate .................. [RUN]
17:03:51  Re-using an available connection from the pool (formerly model.tbd_tpcdi.reference_status_type, now model.tbd_tpcdi.reference_tax_rate)
17:03:51  Began compiling node model.tbd_tpcdi.reference_tax_rate
17:03:51  Writing injected SQL for node "model.tbd_tpcdi.reference_tax_rate"
17:03:51  Timing info for model.tbd_tpcdi.reference_tax_rate (compile): 17:03:51.256901 => 17:03:51.273309
17:03:51  Began executing node model.tbd_tpcdi.reference_tax_rate
17:03:51  Using spark connection "model.tbd_tpcdi.reference_tax_rate"
17:03:51  On model.tbd_tpcdi.reference_tax_rate: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_tax_rate"} */
drop table if exists demo_bronze.reference_tax_rate
17:03:51  Opening a new connection, currently in state closed
17:03:51  SQL status: OK in 0.0 seconds
17:03:51  Writing runtime sql for node "model.tbd_tpcdi.reference_tax_rate"
17:03:51  Spark adapter: NotImplemented: add_begin_query
17:03:51  Using spark connection "model.tbd_tpcdi.reference_tax_rate"
17:03:51  On model.tbd_tpcdi.reference_tax_rate: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_tax_rate"} */

  
    
        create table demo_bronze.reference_tax_rate
      
      
      
      
      
      
      
      

      as
      select *
from digen.tax_rate
  
24/06/20 17:03:51 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:51  SQL status: OK in 1.0 seconds
17:03:51  Timing info for model.tbd_tpcdi.reference_tax_rate (execute): 17:03:51.280437 => 17:03:51.997734
17:03:52  On model.tbd_tpcdi.reference_tax_rate: ROLLBACK
17:03:52  Spark adapter: NotImplemented: rollback
17:03:52  On model.tbd_tpcdi.reference_tax_rate: Close
17:03:52  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d5f0f10>]}
17:03:52  16 of 45 OK created sql table model demo_bronze.reference_tax_rate ............. [OK in 0.76s]
17:03:52  Finished running node model.tbd_tpcdi.reference_tax_rate
17:03:52  Began running node model.tbd_tpcdi.reference_trade_type
17:03:52  17 of 45 START sql table model demo_bronze.reference_trade_type ................ [RUN]
17:03:52  Re-using an available connection from the pool (formerly model.tbd_tpcdi.reference_tax_rate, now model.tbd_tpcdi.reference_trade_type)
17:03:52  Began compiling node model.tbd_tpcdi.reference_trade_type
17:03:52  Writing injected SQL for node "model.tbd_tpcdi.reference_trade_type"
17:03:52  Timing info for model.tbd_tpcdi.reference_trade_type (compile): 17:03:52.032394 => 17:03:52.041811
17:03:52  Began executing node model.tbd_tpcdi.reference_trade_type
17:03:52  Using spark connection "model.tbd_tpcdi.reference_trade_type"
17:03:52  On model.tbd_tpcdi.reference_trade_type: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_trade_type"} */
drop table if exists demo_bronze.reference_trade_type
17:03:52  Opening a new connection, currently in state closed
17:03:52  SQL status: OK in 0.0 seconds
17:03:52  Writing runtime sql for node "model.tbd_tpcdi.reference_trade_type"
17:03:52  Spark adapter: NotImplemented: add_begin_query
17:03:52  Using spark connection "model.tbd_tpcdi.reference_trade_type"
17:03:52  On model.tbd_tpcdi.reference_trade_type: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.reference_trade_type"} */

  
    
        create table demo_bronze.reference_trade_type
      
      
      
      
      
      
      
      

      as
      select *
from digen.trade_type
  
24/06/20 17:03:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:52  SQL status: OK in 1.0 seconds
17:03:52  Timing info for model.tbd_tpcdi.reference_trade_type (execute): 17:03:52.044342 => 17:03:52.817371
17:03:52  On model.tbd_tpcdi.reference_trade_type: ROLLBACK
17:03:52  Spark adapter: NotImplemented: rollback
17:03:52  On model.tbd_tpcdi.reference_trade_type: Close
17:03:52  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c52a760>]}
17:03:52  17 of 45 OK created sql table model demo_bronze.reference_trade_type ........... [OK in 0.80s]
17:03:52  Finished running node model.tbd_tpcdi.reference_trade_type
17:03:52  Began running node model.tbd_tpcdi.syndicated_prospect
17:03:52  18 of 45 START sql table model demo_bronze.syndicated_prospect ................. [RUN]
17:03:52  Re-using an available connection from the pool (formerly model.tbd_tpcdi.reference_trade_type, now model.tbd_tpcdi.syndicated_prospect)
17:03:52  Began compiling node model.tbd_tpcdi.syndicated_prospect
17:03:52  Writing injected SQL for node "model.tbd_tpcdi.syndicated_prospect"
17:03:52  Timing info for model.tbd_tpcdi.syndicated_prospect (compile): 17:03:52.849936 => 17:03:52.861023
17:03:52  Began executing node model.tbd_tpcdi.syndicated_prospect
17:03:52  Using spark connection "model.tbd_tpcdi.syndicated_prospect"
17:03:52  On model.tbd_tpcdi.syndicated_prospect: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.syndicated_prospect"} */
drop table if exists demo_bronze.syndicated_prospect
17:03:52  Opening a new connection, currently in state closed
17:03:52  SQL status: OK in 0.0 seconds
17:03:52  Writing runtime sql for node "model.tbd_tpcdi.syndicated_prospect"
17:03:52  Spark adapter: NotImplemented: add_begin_query
17:03:52  Using spark connection "model.tbd_tpcdi.syndicated_prospect"
17:03:52  On model.tbd_tpcdi.syndicated_prospect: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.syndicated_prospect"} */

  
    
        create table demo_bronze.syndicated_prospect
      
      
      
      
      
      
      
      

      as
      select *
from digen.prospect
  
24/06/20 17:03:53 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:03:55  SQL status: OK in 3.0 seconds
17:03:55  Timing info for model.tbd_tpcdi.syndicated_prospect (execute): 17:03:52.866448 => 17:03:55.855520
17:03:55  On model.tbd_tpcdi.syndicated_prospect: ROLLBACK
17:03:55  Spark adapter: NotImplemented: rollback
17:03:55  On model.tbd_tpcdi.syndicated_prospect: Close
17:03:55  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4472b0>]}
17:03:55  18 of 45 OK created sql table model demo_bronze.syndicated_prospect ............ [OK in 3.02s]
17:03:55  Finished running node model.tbd_tpcdi.syndicated_prospect
17:03:55  Began running node model.tbd_tpcdi.daily_market
17:03:55  19 of 45 START sql table model demo_silver.daily_market ........................ [RUN]
17:03:55  Re-using an available connection from the pool (formerly model.tbd_tpcdi.syndicated_prospect, now model.tbd_tpcdi.daily_market)
17:03:55  Began compiling node model.tbd_tpcdi.daily_market
17:03:55  Writing injected SQL for node "model.tbd_tpcdi.daily_market"
17:03:55  Timing info for model.tbd_tpcdi.daily_market (compile): 17:03:55.887130 => 17:03:55.899233
17:03:55  Began executing node model.tbd_tpcdi.daily_market
17:03:55  Using spark connection "model.tbd_tpcdi.daily_market"
17:03:55  On model.tbd_tpcdi.daily_market: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market"} */
drop table if exists demo_silver.daily_market
17:03:55  Opening a new connection, currently in state closed
17:03:56  SQL status: OK in 0.0 seconds
17:03:56  Writing runtime sql for node "model.tbd_tpcdi.daily_market"
17:03:56  Spark adapter: NotImplemented: add_begin_query
17:03:56  Using spark connection "model.tbd_tpcdi.daily_market"
17:03:56  On model.tbd_tpcdi.daily_market: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market"} */

  
    
        create table demo_silver.daily_market
      
      
      
      
      
      
      
      

      as
      with
    s1 as (
        select
            -- dm_date,
            min(dm_low) over (
                partition by dm_s_symb
                order by dm_date asc
                rows between 364 preceding and 0 following  -- CURRENT ROW
            ) fifty_two_week_low,
            max(dm_high) over (
                partition by dm_s_symb
                order by dm_date asc
                rows between 364 preceding and 0 following  -- CURRENT ROW
            ) fifty_two_week_high,
            *
        from demo_bronze.brokerage_daily_market
    ),
    s2 as (
        select a.*, 
               b.dm_date as fifty_two_week_low_date, 
               c.dm_date as fifty_two_week_high_date
        from s1 a
        join
            s1 b
            on a.dm_s_symb = b.dm_s_symb
            and a.fifty_two_week_low = b.dm_low
            and b.dm_date <= a.dm_date
        join
            s1 c
            on a.dm_s_symb = c.dm_s_symb
            and a.fifty_two_week_high = c.dm_high
            and c.dm_date <= a.dm_date
    )

SELECT * FROM (
  SELECT *, row_number() over (
        partition by dm_s_symb, dm_date
        order by fifty_two_week_low_date, fifty_two_week_high_date
    )rank FROM s2) tmp WHERE rank = 1
  
24/06/20 17:03:56 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:13:37  SQL status: OK in 581.0 seconds
17:13:37  Timing info for model.tbd_tpcdi.daily_market (execute): 17:03:55.904981 => 17:13:37.173645
17:13:37  On model.tbd_tpcdi.daily_market: ROLLBACK
17:13:37  Spark adapter: NotImplemented: rollback
17:13:37  On model.tbd_tpcdi.daily_market: Close
17:13:37  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4f81c0>]}
17:13:37  19 of 45 OK created sql table model demo_silver.daily_market ................... [OK in 581.32s]
17:13:37  Finished running node model.tbd_tpcdi.daily_market
17:13:37  Began running node model.tbd_tpcdi.daily_market-checkpoint
17:13:37  20 of 45 START sql table model demo_silver.daily_market-checkpoint ............. [RUN]
17:13:37  Re-using an available connection from the pool (formerly model.tbd_tpcdi.daily_market, now model.tbd_tpcdi.daily_market-checkpoint)
17:13:37  Began compiling node model.tbd_tpcdi.daily_market-checkpoint
17:13:37  Writing injected SQL for node "model.tbd_tpcdi.daily_market-checkpoint"
17:13:37  Timing info for model.tbd_tpcdi.daily_market-checkpoint (compile): 17:13:37.233288 => 17:13:37.243621
17:13:37  Began executing node model.tbd_tpcdi.daily_market-checkpoint
17:13:37  Using spark connection "model.tbd_tpcdi.daily_market-checkpoint"
17:13:37  On model.tbd_tpcdi.daily_market-checkpoint: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market-checkpoint"} */
drop table if exists demo_silver.daily_market-checkpoint
17:13:37  Opening a new connection, currently in state closed
17:13:37  Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market-checkpoint"} */
drop table if exists demo_silver.daily_market-checkpoint
17:13:37  Spark adapter: 
Possibly unquoted identifier daily_market-checkpoint detected. Please consider quoting it with back-quotes as `daily_market-checkpoint`(line 2, pos 45)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market-checkpoint"} */
drop table if exists demo_silver.daily_market-checkpoint
---------------------------------------------^^^

17:13:37  Spark adapter: Error while running:
macro drop_relation
17:13:37  Spark adapter: 
Possibly unquoted identifier daily_market-checkpoint detected. Please consider quoting it with back-quotes as `daily_market-checkpoint`(line 2, pos 45)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market-checkpoint"} */
drop table if exists demo_silver.daily_market-checkpoint
---------------------------------------------^^^

17:13:37  Timing info for model.tbd_tpcdi.daily_market-checkpoint (execute): 17:13:37.246302 => 17:13:37.321423
17:13:37  On model.tbd_tpcdi.daily_market-checkpoint: Close
17:13:37  Unhandled error while executing 
Possibly unquoted identifier daily_market-checkpoint detected. Please consider quoting it with back-quotes as `daily_market-checkpoint`(line 2, pos 45)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market-checkpoint"} */
drop table if exists demo_silver.daily_market-checkpoint
---------------------------------------------^^^
17:13:37  Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 380, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 326, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 427, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/run.py", line 292, in execute
    result = MacroGenerator(
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 99, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/sql/impl.py", line 142, in drop_relation
    self.execute_macro(DROP_RELATION_MACRO_NAME, kwargs={"relation": relation})
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 1112, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 20, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 29, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 310, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/sql/connections.py", line 138, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/spark/session.py", line 227, in execute
    self._cursor.execute(sql)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/spark/session.py", line 124, in execute
    self._df = spark_session.sql(sql)
  File "/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py", line 1034, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self)
  File "/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py", line 196, in deco
    raise converted from None
pyspark.sql.utils.ParseException: 
Possibly unquoted identifier daily_market-checkpoint detected. Please consider quoting it with back-quotes as `daily_market-checkpoint`(line 2, pos 45)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market-checkpoint"} */
drop table if exists demo_silver.daily_market-checkpoint
---------------------------------------------^^^


17:13:37  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c47f610>]}
17:13:37  20 of 45 ERROR creating sql table model demo_silver.daily_market-checkpoint .... [ERROR in 0.15s]
17:13:37  Finished running node model.tbd_tpcdi.daily_market-checkpoint
17:13:37  Began running node model.tbd_tpcdi.employees
17:13:37  21 of 45 START sql table model demo_silver.employees ........................... [RUN]
17:13:37  Re-using an available connection from the pool (formerly model.tbd_tpcdi.daily_market-checkpoint, now model.tbd_tpcdi.employees)
17:13:37  Began compiling node model.tbd_tpcdi.employees
17:13:37  Writing injected SQL for node "model.tbd_tpcdi.employees"
17:13:37  Timing info for model.tbd_tpcdi.employees (compile): 17:13:37.392399 => 17:13:37.406453
17:13:37  Began executing node model.tbd_tpcdi.employees
17:13:37  Using spark connection "model.tbd_tpcdi.employees"
17:13:37  On model.tbd_tpcdi.employees: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.employees"} */
drop table if exists demo_silver.employees
17:13:37  Opening a new connection, currently in state closed
17:13:37  SQL status: OK in 0.0 seconds
17:13:37  Writing runtime sql for node "model.tbd_tpcdi.employees"
17:13:37  Spark adapter: NotImplemented: add_begin_query
17:13:37  Using spark connection "model.tbd_tpcdi.employees"
17:13:37  On model.tbd_tpcdi.employees: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.employees"} */

  
    
        create table demo_silver.employees
      
      
      
      
      
      
      
      

      as
      select 
    employee_id,
    manager_id,
    employee_first_name first_name,
    employee_last_name last_name,
    employee_mi middle_initial,
    employee_job_code job_code,
    employee_branch branch,
    employee_office office,
    employee_phone phone
from demo_bronze.hr_employee
  
24/06/20 17:13:37 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:13:40  SQL status: OK in 2.0 seconds
17:13:40  Timing info for model.tbd_tpcdi.employees (execute): 17:13:37.410694 => 17:13:40.135611
17:13:40  On model.tbd_tpcdi.employees: ROLLBACK
17:13:40  Spark adapter: NotImplemented: rollback
17:13:40  On model.tbd_tpcdi.employees: Close
17:13:40  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c414cd0>]}
17:13:40  21 of 45 OK created sql table model demo_silver.employees ...................... [OK in 2.77s]
17:13:40  Finished running node model.tbd_tpcdi.employees
17:13:40  Began running node model.tbd_tpcdi.date
17:13:40  22 of 45 START sql table model demo_silver.date ................................ [RUN]
17:13:40  Re-using an available connection from the pool (formerly model.tbd_tpcdi.employees, now model.tbd_tpcdi.date)
17:13:40  Began compiling node model.tbd_tpcdi.date
17:13:40  Writing injected SQL for node "model.tbd_tpcdi.date"
17:13:40  Timing info for model.tbd_tpcdi.date (compile): 17:13:40.191416 => 17:13:40.201246
17:13:40  Began executing node model.tbd_tpcdi.date
17:13:40  Using spark connection "model.tbd_tpcdi.date"
17:13:40  On model.tbd_tpcdi.date: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.date"} */
drop table if exists demo_silver.date
17:13:40  Opening a new connection, currently in state closed
17:13:40  SQL status: OK in 0.0 seconds
17:13:40  Writing runtime sql for node "model.tbd_tpcdi.date"
17:13:40  Spark adapter: NotImplemented: add_begin_query
17:13:40  Using spark connection "model.tbd_tpcdi.date"
17:13:40  On model.tbd_tpcdi.date: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.date"} */

  
    
        create table demo_silver.date
      
      
      
      
      
      
      
      

      as
      select *
from demo_bronze.reference_date
  
24/06/20 17:13:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:13:41  SQL status: OK in 1.0 seconds
17:13:41  Timing info for model.tbd_tpcdi.date (execute): 17:13:40.204631 => 17:13:41.461103
17:13:41  On model.tbd_tpcdi.date: ROLLBACK
17:13:41  Spark adapter: NotImplemented: rollback
17:13:41  On model.tbd_tpcdi.date: Close
17:13:41  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c47f520>]}
17:13:41  22 of 45 OK created sql table model demo_silver.date ........................... [OK in 1.30s]
17:13:41  Finished running node model.tbd_tpcdi.date
17:13:41  Began running node model.tbd_tpcdi.companies
17:13:41  23 of 45 START sql table model demo_silver.companies ........................... [RUN]
17:13:41  Re-using an available connection from the pool (formerly model.tbd_tpcdi.date, now model.tbd_tpcdi.companies)
17:13:41  Began compiling node model.tbd_tpcdi.companies
17:13:41  Writing injected SQL for node "model.tbd_tpcdi.companies"
17:13:41  Timing info for model.tbd_tpcdi.companies (compile): 17:13:41.506546 => 17:13:41.517001
17:13:41  Began executing node model.tbd_tpcdi.companies
17:13:41  Using spark connection "model.tbd_tpcdi.companies"
17:13:41  On model.tbd_tpcdi.companies: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.companies"} */
drop table if exists demo_silver.companies
17:13:41  Opening a new connection, currently in state closed
17:13:41  SQL status: OK in 0.0 seconds
17:13:41  Writing runtime sql for node "model.tbd_tpcdi.companies"
17:13:41  Spark adapter: NotImplemented: add_begin_query
17:13:41  Using spark connection "model.tbd_tpcdi.companies"
17:13:41  On model.tbd_tpcdi.companies: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.companies"} */

  
    
        create table demo_silver.companies
      
      
      
      
      
      
      
      

      as
      select
    cik as company_id,
    st.st_name status,
    company_name name,
    ind.in_name industry,
    ceo_name ceo,
    address_line1,
    address_line2,
    postal_code,
    city,
    state_province,
    country,
    description,
    founding_date,
    sp_rating,
    pts as effective_timestamp,
     ifnull(
            lag(pts) over (
                partition by cik
                order by
                pts desc
            ) - INTERVAL 1 milliseconds,
        to_timestamp('9999-12-31 23:59:59.999')
    ) as end_timestamp,
    CASE
        WHEN (
            row_number() over (
                partition by cik
                order by
                pts desc
            ) = 1
        ) THEN TRUE
        ELSE FALSE
    END as IS_CURRENT
from demo_bronze.finwire_company cmp
join demo_bronze.reference_status_type st on cmp.status = st.st_id
join demo_bronze.reference_industry ind on cmp.industry_id = ind.in_id
  
24/06/20 17:13:42 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:13:46  SQL status: OK in 5.0 seconds
17:13:46  Timing info for model.tbd_tpcdi.companies (execute): 17:13:41.520406 => 17:13:46.658071
17:13:46  On model.tbd_tpcdi.companies: ROLLBACK
17:13:46  Spark adapter: NotImplemented: rollback
17:13:46  On model.tbd_tpcdi.companies: Close
17:13:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4c5070>]}
17:13:47  23 of 45 OK created sql table model demo_silver.companies ...................... [OK in 5.17s]
17:13:47  Finished running node model.tbd_tpcdi.companies
17:13:47  Began running node model.tbd_tpcdi.accounts
17:13:47  24 of 45 START sql table model demo_silver.accounts ............................ [RUN]
17:13:47  Re-using an available connection from the pool (formerly model.tbd_tpcdi.companies, now model.tbd_tpcdi.accounts)
17:13:47  Began compiling node model.tbd_tpcdi.accounts
17:13:47  Writing injected SQL for node "model.tbd_tpcdi.accounts"
17:13:47  Timing info for model.tbd_tpcdi.accounts (compile): 17:13:47.355155 => 17:13:47.364318
17:13:47  Began executing node model.tbd_tpcdi.accounts
17:13:47  Using spark connection "model.tbd_tpcdi.accounts"
17:13:47  On model.tbd_tpcdi.accounts: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.accounts"} */
drop table if exists demo_silver.accounts
17:13:47  Opening a new connection, currently in state closed
17:13:47  SQL status: OK in 0.0 seconds
17:13:47  Writing runtime sql for node "model.tbd_tpcdi.accounts"
17:13:47  Spark adapter: NotImplemented: add_begin_query
17:13:47  Using spark connection "model.tbd_tpcdi.accounts"
17:13:47  On model.tbd_tpcdi.accounts: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.accounts"} */

  
    
        create table demo_silver.accounts
      
      
      
      
      
      
      
      

      as
      select
    action_type,
    decode(action_type,
      'NEW','Active',
      'ADDACCT','Active',
      'UPDACCT','Active',
      'CLOSEACCT','Inactive') status,
    ca_id account_id,
    ca_name account_desc,
    c_id customer_id,
    c_tax_id tax_id,
    c_gndr gender,
    c_tier tier,
    c_dob dob,
    c_l_name last_name,
    c_f_name first_name,
    c_m_name middle_name,
    c_adline1 address_line1,
    c_adline2 address_line2,
    c_zipcode postal_code,
    c_city city,
    c_state_prov state_province,
    c_ctry country,
    c_prim_email primary_email,
    c_alt_email alternate_email,
    c_phone_1 phone1,
    c_phone_2 phone2,
    c_phone_3 phone3,
    c_lcl_tx_id local_tax_rate_name,
    ltx.tx_rate local_tax_rate,
    c_nat_tx_id national_tax_rate_name,
    ntx.tx_rate national_tax_rate,
    ca_tax_st tax_status,
    ca_b_id broker_id,
    action_ts as effective_timestamp,
    ifnull(
        lag(action_ts) over (
            partition by ca_id
            order by
            action_ts desc
        ) - INTERVAL 1 milliseconds,
        to_timestamp('9999-12-31 23:59:59.999')
    ) as end_timestamp,
    CASE
        WHEN (
            row_number() over (
                partition by ca_id
                order by
                action_ts desc
            ) = 1
        ) THEN TRUE
        ELSE FALSE
    END as IS_CURRENT
from
    demo_bronze.crm_customer_mgmt c
left join
    demo_bronze.reference_tax_rate ntx
on
    c.c_nat_tx_id = ntx.tx_id
left join
    demo_bronze.reference_tax_rate ltx
on
    c.c_lcl_tx_id = ltx.tx_id
where ca_id is not null
  
24/06/20 17:13:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:13:57  SQL status: OK in 10.0 seconds
17:13:57  Timing info for model.tbd_tpcdi.accounts (execute): 17:13:47.369513 => 17:13:57.232063
17:13:57  On model.tbd_tpcdi.accounts: ROLLBACK
17:13:57  Spark adapter: NotImplemented: rollback
17:13:57  On model.tbd_tpcdi.accounts: Close
17:13:57  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c523cd0>]}
17:13:57  24 of 45 OK created sql table model demo_silver.accounts ....................... [OK in 9.90s]
17:13:57  Finished running node model.tbd_tpcdi.accounts
17:13:57  Began running node model.tbd_tpcdi.customers
17:13:57  25 of 45 START sql table model demo_silver.customers ........................... [RUN]
17:13:57  Re-using an available connection from the pool (formerly model.tbd_tpcdi.accounts, now model.tbd_tpcdi.customers)
17:13:57  Began compiling node model.tbd_tpcdi.customers
17:13:57  Writing injected SQL for node "model.tbd_tpcdi.customers"
17:13:57  Timing info for model.tbd_tpcdi.customers (compile): 17:13:57.288418 => 17:13:57.308936
17:13:57  Began executing node model.tbd_tpcdi.customers
17:13:57  Using spark connection "model.tbd_tpcdi.customers"
17:13:57  On model.tbd_tpcdi.customers: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.customers"} */
drop table if exists demo_silver.customers
17:13:57  Opening a new connection, currently in state closed
17:13:57  SQL status: OK in 0.0 seconds
17:13:57  Writing runtime sql for node "model.tbd_tpcdi.customers"
17:13:57  Spark adapter: NotImplemented: add_begin_query
17:13:57  Using spark connection "model.tbd_tpcdi.customers"
17:13:57  On model.tbd_tpcdi.customers: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.customers"} */

  
    
        create table demo_silver.customers
      
      
      
      
      
      
      
      

      as
      select
    action_type,
    decode(action_type,
      'NEW','Active',
      'ADDACCT','Active',
      'UPDACCT','Active',
      'UPDCUST','Active',
      'INACT','Inactive') status,
    c_id customer_id,
    ca_id account_id,
    c_tax_id tax_id,
    c_gndr gender,
    c_tier tier,
    c_dob dob,
    c_l_name last_name,
    c_f_name first_name,
    c_m_name middle_name,
    c_adline1 address_line1,
    c_adline2 address_line2,
    c_zipcode postal_code,
    c_city city,
    c_state_prov state_province,
    c_ctry country,
    c_prim_email primary_email,
    c_alt_email alternate_email,
    c_phone_1 phone1,
    c_phone_2 phone2,
    c_phone_3 phone3,
    c_lcl_tx_id local_tax_rate_name,
    ltx.tx_rate local_tax_rate,
    c_nat_tx_id national_tax_rate_name,
    ntx.tx_rate national_tax_rate,
    ca_tax_st account_tax_status,
    ca_b_id broker_id,
    action_ts as effective_timestamp,
    ifnull(
            lag(action_ts) over (
                partition by c_id
                order by
                action_ts desc
            ) - INTERVAL 1 milliseconds,
        to_timestamp('9999-12-31 23:59:59.999')
    ) as end_timestamp,
    CASE
        WHEN (
            row_number() over (
                partition by c_id
                order by
                action_ts desc
            ) = 1
        ) THEN TRUE
        ELSE FALSE
    END as IS_CURRENT
from
    demo_bronze.crm_customer_mgmt c
left join
    demo_bronze.reference_tax_rate ntx
on
    c.c_nat_tx_id = ntx.tx_id
left join
    demo_bronze.reference_tax_rate ltx
on
    c.c_lcl_tx_id = ltx.tx_id
where action_type in ('NEW', 'INACT', 'UPDCUST')
  
24/06/20 17:13:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:14:05  SQL status: OK in 8.0 seconds
17:14:05  Timing info for model.tbd_tpcdi.customers (execute): 17:13:57.314779 => 17:14:05.243960
17:14:05  On model.tbd_tpcdi.customers: ROLLBACK
17:14:05  Spark adapter: NotImplemented: rollback
17:14:05  On model.tbd_tpcdi.customers: Close
17:14:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d64b3d0>]}
17:14:05  25 of 45 OK created sql table model demo_silver.customers ...................... [OK in 7.98s]
17:14:05  Finished running node model.tbd_tpcdi.customers
17:14:05  Began running node model.tbd_tpcdi.trades_history
17:14:05  26 of 45 START sql table model demo_silver.trades_history ...................... [RUN]
17:14:05  Re-using an available connection from the pool (formerly model.tbd_tpcdi.customers, now model.tbd_tpcdi.trades_history)
17:14:05  Began compiling node model.tbd_tpcdi.trades_history
17:14:05  Writing injected SQL for node "model.tbd_tpcdi.trades_history"
17:14:05  Timing info for model.tbd_tpcdi.trades_history (compile): 17:14:05.275910 => 17:14:05.289342
17:14:05  Began executing node model.tbd_tpcdi.trades_history
17:14:05  Using spark connection "model.tbd_tpcdi.trades_history"
17:14:05  On model.tbd_tpcdi.trades_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.trades_history"} */
drop table if exists demo_silver.trades_history
17:14:05  Opening a new connection, currently in state closed
17:14:05  SQL status: OK in 0.0 seconds
17:14:05  Writing runtime sql for node "model.tbd_tpcdi.trades_history"
17:14:05  Spark adapter: NotImplemented: add_begin_query
17:14:05  Using spark connection "model.tbd_tpcdi.trades_history"
17:14:05  On model.tbd_tpcdi.trades_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.trades_history"} */

  
    
        create table demo_silver.trades_history
      
      
      
      
      
      
      
      

      as
      select
    t_id trade_id,
    t_dts trade_timestamp,
    t_ca_id account_id,
    ts.st_name trade_status,
    tt_name trade_type,
    case t_is_cash
        when true then 'Cash'
        when false then 'Margin'
    end transaction_type,
    t_s_symb symbol,
    t_exec_name executor_name,
    t_qty quantity,
    t_bid_price bid_price,
    t_trade_price trade_price,
    t_chrg fee,
    t_comm commission,
    t_tax tax,
    us.st_name update_status,
    th_dts effective_timestamp,
    ifnull(
        lag(th_dts) over (
            partition by t_id
            order by
            th_dts desc
        ) - INTERVAL 1 milliseconds,
        to_timestamp('9999-12-31 23:59:59.999')
    ) as end_timestamp,
    CASE
        WHEN (
            row_number() over (
                partition by t_id
                order by
                th_dts desc
            ) = 1
        ) THEN TRUE
        ELSE FALSE
    END as IS_CURRENT
from
    demo_bronze.brokerage_trade 
join
    demo_bronze.brokerage_trade_history  
on
    t_id = th_t_id
join
    demo_bronze.reference_trade_type 
on
    t_tt_id = tt_id
join
    demo_bronze.reference_status_type ts
on
    t_st_id = ts.st_id
join
    demo_bronze.reference_status_type us
on th_st_id = us.st_id
  
24/06/20 17:14:05 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:16:55  SQL status: OK in 170.0 seconds
17:16:55  Timing info for model.tbd_tpcdi.trades_history (execute): 17:14:05.295011 => 17:16:55.681415
17:16:55  On model.tbd_tpcdi.trades_history: ROLLBACK
17:16:55  Spark adapter: NotImplemented: rollback
17:16:55  On model.tbd_tpcdi.trades_history: Close
17:16:55  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7de65280>]}
17:16:55  26 of 45 OK created sql table model demo_silver.trades_history ................. [OK in 170.44s]
17:16:55  Finished running node model.tbd_tpcdi.trades_history
17:16:55  Began running node model.tbd_tpcdi.dim_broker
17:16:55  27 of 45 START sql table model demo_gold.dim_broker ............................ [RUN]
17:16:55  Re-using an available connection from the pool (formerly model.tbd_tpcdi.trades_history, now model.tbd_tpcdi.dim_broker)
17:16:55  Began compiling node model.tbd_tpcdi.dim_broker
17:16:55  Writing injected SQL for node "model.tbd_tpcdi.dim_broker"
17:16:55  Timing info for model.tbd_tpcdi.dim_broker (compile): 17:16:55.734714 => 17:16:55.763052
17:16:55  Began executing node model.tbd_tpcdi.dim_broker
17:16:55  Using spark connection "model.tbd_tpcdi.dim_broker"
17:16:55  On model.tbd_tpcdi.dim_broker: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_broker"} */
drop table if exists demo_gold.dim_broker
17:16:55  Opening a new connection, currently in state closed
17:16:55  SQL status: OK in 0.0 seconds
17:16:56  Writing runtime sql for node "model.tbd_tpcdi.dim_broker"
17:16:56  Spark adapter: NotImplemented: add_begin_query
17:16:56  Using spark connection "model.tbd_tpcdi.dim_broker"
17:16:56  On model.tbd_tpcdi.dim_broker: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_broker"} */

  
    
        create table demo_gold.dim_broker
      
      
      
      
      
      
      
      

      as
      select
    md5(cast(concat(coalesce(cast(employee_id as string), '_dbt_utils_surrogate_key_null_')) as string)) sk_broker_id,
    employee_id broker_id,
    manager_id,
    first_name,
    last_name,
    middle_initial,
    job_code,
    branch,
    office,
    phone
from
    demo_silver.employees
  
24/06/20 17:16:56 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:16:58  SQL status: OK in 2.0 seconds
17:16:58  Timing info for model.tbd_tpcdi.dim_broker (execute): 17:16:55.765603 => 17:16:58.453479
17:16:58  On model.tbd_tpcdi.dim_broker: ROLLBACK
17:16:58  Spark adapter: NotImplemented: rollback
17:16:58  On model.tbd_tpcdi.dim_broker: Close
17:16:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c556d00>]}
17:16:58  27 of 45 OK created sql table model demo_gold.dim_broker ....................... [OK in 2.74s]
17:16:58  Finished running node model.tbd_tpcdi.dim_broker
17:16:58  Began running node model.tbd_tpcdi.dim_date
17:16:58  28 of 45 START sql table model demo_gold.dim_date .............................. [RUN]
17:16:58  Re-using an available connection from the pool (formerly model.tbd_tpcdi.dim_broker, now model.tbd_tpcdi.dim_date)
17:16:58  Began compiling node model.tbd_tpcdi.dim_date
17:16:58  Writing injected SQL for node "model.tbd_tpcdi.dim_date"
17:16:58  Timing info for model.tbd_tpcdi.dim_date (compile): 17:16:58.477784 => 17:16:58.487835
17:16:58  Began executing node model.tbd_tpcdi.dim_date
17:16:58  Using spark connection "model.tbd_tpcdi.dim_date"
17:16:58  On model.tbd_tpcdi.dim_date: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_date"} */
drop table if exists demo_gold.dim_date
17:16:58  Opening a new connection, currently in state closed
17:16:58  SQL status: OK in 0.0 seconds
17:16:58  Writing runtime sql for node "model.tbd_tpcdi.dim_date"
17:16:58  Spark adapter: NotImplemented: add_begin_query
17:16:58  Using spark connection "model.tbd_tpcdi.dim_date"
17:16:58  On model.tbd_tpcdi.dim_date: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_date"} */

  
    
        create table demo_gold.dim_date
      
      
      
      
      
      
      
      

      as
      select *
from demo_silver.date
  
24/06/20 17:16:58 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:16:59  SQL status: OK in 1.0 seconds
17:16:59  Timing info for model.tbd_tpcdi.dim_date (execute): 17:16:58.490354 => 17:16:59.882018
17:16:59  On model.tbd_tpcdi.dim_date: ROLLBACK
17:16:59  Spark adapter: NotImplemented: rollback
17:16:59  On model.tbd_tpcdi.dim_date: Close
17:16:59  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4dec10>]}
17:16:59  28 of 45 OK created sql table model demo_gold.dim_date ......................... [OK in 1.43s]
17:16:59  Finished running node model.tbd_tpcdi.dim_date
17:16:59  Began running node model.tbd_tpcdi.dim_company
17:16:59  29 of 45 START sql table model demo_gold.dim_company ........................... [RUN]
17:16:59  Re-using an available connection from the pool (formerly model.tbd_tpcdi.dim_date, now model.tbd_tpcdi.dim_company)
17:16:59  Began compiling node model.tbd_tpcdi.dim_company
17:16:59  Writing injected SQL for node "model.tbd_tpcdi.dim_company"
17:16:59  Timing info for model.tbd_tpcdi.dim_company (compile): 17:16:59.923195 => 17:16:59.940140
17:16:59  Began executing node model.tbd_tpcdi.dim_company
17:16:59  Using spark connection "model.tbd_tpcdi.dim_company"
17:16:59  On model.tbd_tpcdi.dim_company: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_company"} */
drop table if exists demo_gold.dim_company
17:16:59  Opening a new connection, currently in state closed
17:17:00  SQL status: OK in 0.0 seconds
17:17:00  Writing runtime sql for node "model.tbd_tpcdi.dim_company"
17:17:00  Spark adapter: NotImplemented: add_begin_query
17:17:00  Using spark connection "model.tbd_tpcdi.dim_company"
17:17:00  On model.tbd_tpcdi.dim_company: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_company"} */

  
    
        create table demo_gold.dim_company
      
      
      
      
      
      
      
      

      as
      select
    md5(cast(concat(coalesce(cast(company_id as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(effective_timestamp as string), '_dbt_utils_surrogate_key_null_')) as string)) sk_company_id,
    company_id,
    status,
    name,
    industry,
    ceo,
    address_line1,
    address_line2,
    postal_code,
    city,
    state_province,
    country,
    description,
    founding_date,
    sp_rating,
    case
        when
            sp_rating in (
                'BB',
                'B',
                'CCC',
                'CC',
                'C',
                'D',
                'BB+',
                'B+',
                'CCC+',
                'BB-',
                'B-',
                'CCC-'
            )
        then true
        else false
    end as is_lowgrade,
    effective_timestamp,
    end_timestamp,
    is_current
from demo_silver.companies
  
24/06/20 17:17:00 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:17:02  SQL status: OK in 2.0 seconds
17:17:02  Timing info for model.tbd_tpcdi.dim_company (execute): 17:16:59.942734 => 17:17:02.425538
17:17:02  On model.tbd_tpcdi.dim_company: ROLLBACK
17:17:02  Spark adapter: NotImplemented: rollback
17:17:02  On model.tbd_tpcdi.dim_company: Close
17:17:02  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c52a4f0>]}
17:17:02  29 of 45 OK created sql table model demo_gold.dim_company ...................... [OK in 2.51s]
17:17:02  Finished running node model.tbd_tpcdi.dim_company
17:17:02  Began running node model.tbd_tpcdi.financials
17:17:02  30 of 45 START sql table model demo_silver.financials .......................... [RUN]
17:17:02  Re-using an available connection from the pool (formerly model.tbd_tpcdi.dim_company, now model.tbd_tpcdi.financials)
17:17:02  Began compiling node model.tbd_tpcdi.financials
17:17:02  Writing injected SQL for node "model.tbd_tpcdi.financials"
17:17:02  Timing info for model.tbd_tpcdi.financials (compile): 17:17:02.457977 => 17:17:02.482127
17:17:02  Began executing node model.tbd_tpcdi.financials
17:17:02  Using spark connection "model.tbd_tpcdi.financials"
17:17:02  On model.tbd_tpcdi.financials: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.financials"} */
drop table if exists demo_silver.financials
17:17:02  Opening a new connection, currently in state closed
17:17:02  SQL status: OK in 0.0 seconds
17:17:02  Writing runtime sql for node "model.tbd_tpcdi.financials"
17:17:02  Spark adapter: NotImplemented: add_begin_query
17:17:02  Using spark connection "model.tbd_tpcdi.financials"
17:17:02  On model.tbd_tpcdi.financials: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.financials"} */

  
    
        create table demo_silver.financials
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select
        YEAR,
        QUARTER,
        QUARTER_START_DATE,
        POSTING_DATE,
        REVENUE,
        EARNINGS,
        EPS,
        DILUTED_EPS,
        MARGIN,
        INVENTORY,
        ASSETS,
        LIABILITIES,
        SH_OUT,
        DILUTED_SH_OUT,
        coalesce(c1.name,c2.name) company_name,
        coalesce(c1.company_id, c2.company_id) company_id,
        pts as effective_timestamp
    from demo_bronze.finwire_financial s 
    left join demo_silver.companies c1
    on s.cik = c1.company_id
    and pts between c1.effective_timestamp and c1.end_timestamp
    left join demo_silver.companies c2
    on s.company_name = c2.name
    and pts between c2.effective_timestamp and c2.end_timestamp
)
select
    *,
    ifnull(
        lag(effective_timestamp) over (
            partition by company_id
            order by
            effective_timestamp desc
        ) - INTERVAL 1 milliseconds,
        to_timestamp('9999-12-31 23:59:59.999')
    ) as end_timestamp,
    CASE
        WHEN (
            row_number() over (
                partition by company_id
                order by
                effective_timestamp desc
            ) = 1
        ) THEN TRUE
        ELSE FALSE
    END as IS_CURRENT
from s1
  
24/06/20 17:17:03 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:17:51  SQL status: OK in 49.0 seconds
17:17:51  Timing info for model.tbd_tpcdi.financials (execute): 17:17:02.490590 => 17:17:51.948987
17:17:51  On model.tbd_tpcdi.financials: ROLLBACK
17:17:51  Spark adapter: NotImplemented: rollback
17:17:51  On model.tbd_tpcdi.financials: Close
17:17:51  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c547820>]}
17:17:51  30 of 45 OK created sql table model demo_silver.financials ..................... [OK in 49.52s]
17:17:51  Finished running node model.tbd_tpcdi.financials
17:17:51  Began running node model.tbd_tpcdi.securities
17:17:51  31 of 45 START sql table model demo_silver.securities .......................... [RUN]
17:17:51  Re-using an available connection from the pool (formerly model.tbd_tpcdi.financials, now model.tbd_tpcdi.securities)
17:17:51  Began compiling node model.tbd_tpcdi.securities
17:17:52  Writing injected SQL for node "model.tbd_tpcdi.securities"
17:17:52  Timing info for model.tbd_tpcdi.securities (compile): 17:17:51.991058 => 17:17:52.014292
17:17:52  Began executing node model.tbd_tpcdi.securities
17:17:52  Using spark connection "model.tbd_tpcdi.securities"
17:17:52  On model.tbd_tpcdi.securities: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.securities"} */
drop table if exists demo_silver.securities
17:17:52  Opening a new connection, currently in state closed
17:17:52  SQL status: OK in 0.0 seconds
17:17:52  Writing runtime sql for node "model.tbd_tpcdi.securities"
17:17:52  Spark adapter: NotImplemented: add_begin_query
17:17:52  Using spark connection "model.tbd_tpcdi.securities"
17:17:52  On model.tbd_tpcdi.securities: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.securities"} */

  
    
        create table demo_silver.securities
      
      
      
      
      
      
      
      

      as
      select
    symbol,
    issue_type,
    case s.status
        when 'ACTV' then 'Active'
        when 'INAC' then 'Inactive'
        else null
    end status,
    s.name,
    ex_id exchange_id,
    sh_out shares_outstanding,
    first_trade_date,
    first_exchange_date,
    dividend,
    coalesce(c1.name,c2.name) company_name,
    coalesce(c1.company_id, c2.company_id) company_id,
    pts as effective_timestamp,
    ifnull(
        lag(pts) over (
            partition by symbol
            order by
            pts desc
        ) - INTERVAL 1 milliseconds,
        to_timestamp('9999-12-31 23:59:59.999')
    ) as end_timestamp,
    CASE
        WHEN (
            row_number() over (
                partition by symbol
                order by
                pts desc
            ) = 1
        ) THEN TRUE
        ELSE FALSE
    END as IS_CURRENT
from demo_bronze.finwire_security s 
left join demo_silver.companies c1
on s.cik = c1.company_id
and pts between c1.effective_timestamp and c1.end_timestamp
left join demo_silver.companies c2
on s.company_name = c2.name
and pts between c2.effective_timestamp and c2.end_timestamp
  
24/06/20 17:17:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:17:56  SQL status: OK in 4.0 seconds
17:17:56  Timing info for model.tbd_tpcdi.securities (execute): 17:17:52.018470 => 17:17:56.704647
17:17:56  On model.tbd_tpcdi.securities: ROLLBACK
17:17:56  Spark adapter: NotImplemented: rollback
17:17:56  On model.tbd_tpcdi.securities: Close
17:17:56  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c47cf40>]}
17:17:56  31 of 45 OK created sql table model demo_silver.securities ..................... [OK in 4.73s]
17:17:56  Finished running node model.tbd_tpcdi.securities
17:17:56  Began running node model.tbd_tpcdi.cash_transactions
17:17:56  32 of 45 START sql table model demo_silver.cash_transactions ................... [RUN]
17:17:56  Re-using an available connection from the pool (formerly model.tbd_tpcdi.securities, now model.tbd_tpcdi.cash_transactions)
17:17:56  Began compiling node model.tbd_tpcdi.cash_transactions
17:17:56  Writing injected SQL for node "model.tbd_tpcdi.cash_transactions"
17:17:56  Timing info for model.tbd_tpcdi.cash_transactions (compile): 17:17:56.722214 => 17:17:56.732244
17:17:56  Began executing node model.tbd_tpcdi.cash_transactions
17:17:56  Using spark connection "model.tbd_tpcdi.cash_transactions"
17:17:56  On model.tbd_tpcdi.cash_transactions: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.cash_transactions"} */
drop table if exists demo_silver.cash_transactions
17:17:56  Opening a new connection, currently in state closed
17:17:56  SQL status: OK in 0.0 seconds
17:17:56  Writing runtime sql for node "model.tbd_tpcdi.cash_transactions"
17:17:56  Spark adapter: NotImplemented: add_begin_query
17:17:56  Using spark connection "model.tbd_tpcdi.cash_transactions"
17:17:56  On model.tbd_tpcdi.cash_transactions: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.cash_transactions"} */

  
    
        create table demo_silver.cash_transactions
      
      
      
      
      
      
      
      

      as
      with t as (
    select
        ct_ca_id account_id,
        ct_dts transaction_timestamp,
        ct_amt amount,
        ct_name description
    from
        demo_bronze.brokerage_cash_transaction
)
select
    a.customer_id,
    t.*
from
    t
join
    demo_silver.accounts a
on
    t.account_id = a.account_id
and
    t.transaction_timestamp between a.effective_timestamp and a.end_timestamp
  
24/06/20 17:17:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:18:22  SQL status: OK in 26.0 seconds
17:18:22  Timing info for model.tbd_tpcdi.cash_transactions (execute): 17:17:56.734872 => 17:18:22.970616
17:18:22  On model.tbd_tpcdi.cash_transactions: ROLLBACK
17:18:22  Spark adapter: NotImplemented: rollback
17:18:22  On model.tbd_tpcdi.cash_transactions: Close
17:18:22  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c41be80>]}
17:18:23  32 of 45 OK created sql table model demo_silver.cash_transactions .............. [OK in 26.27s]
17:18:23  Finished running node model.tbd_tpcdi.cash_transactions
17:18:23  Began running node model.tbd_tpcdi.dim_customer
17:18:23  33 of 45 START sql table model demo_gold.dim_customer .......................... [RUN]
17:18:23  Re-using an available connection from the pool (formerly model.tbd_tpcdi.cash_transactions, now model.tbd_tpcdi.dim_customer)
17:18:23  Began compiling node model.tbd_tpcdi.dim_customer
17:18:23  Writing injected SQL for node "model.tbd_tpcdi.dim_customer"
17:18:23  Timing info for model.tbd_tpcdi.dim_customer (compile): 17:18:23.024685 => 17:18:23.043191
17:18:23  Began executing node model.tbd_tpcdi.dim_customer
17:18:23  Using spark connection "model.tbd_tpcdi.dim_customer"
17:18:23  On model.tbd_tpcdi.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_customer"} */
drop table if exists demo_gold.dim_customer
17:18:23  Opening a new connection, currently in state closed
17:18:23  SQL status: OK in 0.0 seconds
17:18:23  Writing runtime sql for node "model.tbd_tpcdi.dim_customer"
17:18:23  Spark adapter: NotImplemented: add_begin_query
17:18:23  Using spark connection "model.tbd_tpcdi.dim_customer"
17:18:23  On model.tbd_tpcdi.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_customer"} */

  
    
        create table demo_gold.dim_customer
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select c.*,
           p.agency_id,
           p.credit_rating,
           p.net_worth
    FROM demo_silver.customers c
    left join demo_bronze.syndicated_prospect p
    using (first_name, last_name, postal_code, address_line1, address_line2)
),
s2 as (
    SELECT
        md5(cast(concat(coalesce(cast(customer_id as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(effective_timestamp as string), '_dbt_utils_surrogate_key_null_')) as string)) sk_customer_id,
        customer_id,
        coalesce(tax_id, last_value(tax_id) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) tax_id,
        status,
        coalesce(last_name, last_value(last_name) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) last_name,
        coalesce(first_name, last_value(first_name) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) first_name,
        coalesce(middle_name, last_value(middle_name) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) middleinitial,
        coalesce(gender, last_value(gender) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) gender,
        coalesce(tier, last_value(tier) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) tier,
        coalesce(dob, last_value(dob) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) dob,
        coalesce(address_line1, last_value(address_line1) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) address_line1,
        coalesce(address_line2, last_value(address_line2) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) address_line2,
        coalesce(postal_code, last_value(postal_code) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) postal_code,
        coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) CITY,
        coalesce(state_province, last_value(state_province) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) state_province,
        coalesce(country, last_value(country) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) country,
        coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) phone1,
        coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) phone2,
        coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) phone3,
        coalesce(primary_email, last_value(primary_email) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) primary_email,
        coalesce(alternate_email, last_value(alternate_email) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) alternate_email,
        coalesce(local_tax_rate_name, last_value(local_tax_rate_name) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) local_tax_rate_name,
        coalesce(local_tax_rate, last_value(local_tax_rate) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) local_tax_rate,
        coalesce(national_tax_rate_name, last_value(national_tax_rate_name) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) national_tax_rate_name,
        coalesce(national_tax_rate, last_value(national_tax_rate) IGNORE NULLS OVER (
            PARTITION BY customer_id
            ORDER BY effective_timestamp)) national_tax_rate,
        agency_id,
        credit_rating,
        net_worth,
        effective_timestamp,
        end_timestamp,
        is_current
    FROM s1
)
select *
from s2
  
24/06/20 17:18:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:18:38  SQL status: OK in 15.0 seconds
17:18:38  Timing info for model.tbd_tpcdi.dim_customer (execute): 17:18:23.046875 => 17:18:38.338827
17:18:38  On model.tbd_tpcdi.dim_customer: ROLLBACK
17:18:38  Spark adapter: NotImplemented: rollback
17:18:38  On model.tbd_tpcdi.dim_customer: Close
17:18:38  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c45b130>]}
17:18:38  33 of 45 OK created sql table model demo_gold.dim_customer ..................... [OK in 15.34s]
17:18:38  Finished running node model.tbd_tpcdi.dim_customer
17:18:38  Began running node model.tbd_tpcdi.dim_trade
17:18:38  34 of 45 START sql table model demo_gold.dim_trade ............................. [RUN]
17:18:38  Re-using an available connection from the pool (formerly model.tbd_tpcdi.dim_customer, now model.tbd_tpcdi.dim_trade)
17:18:38  Began compiling node model.tbd_tpcdi.dim_trade
17:18:38  Writing injected SQL for node "model.tbd_tpcdi.dim_trade"
17:18:38  Timing info for model.tbd_tpcdi.dim_trade (compile): 17:18:38.372638 => 17:18:38.388840
17:18:38  Began executing node model.tbd_tpcdi.dim_trade
17:18:38  Using spark connection "model.tbd_tpcdi.dim_trade"
17:18:38  On model.tbd_tpcdi.dim_trade: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_trade"} */
drop table if exists demo_gold.dim_trade
17:18:38  Opening a new connection, currently in state closed
17:18:38  SQL status: OK in 0.0 seconds
17:18:38  Writing runtime sql for node "model.tbd_tpcdi.dim_trade"
17:18:38  Spark adapter: NotImplemented: add_begin_query
17:18:38  Using spark connection "model.tbd_tpcdi.dim_trade"
17:18:38  On model.tbd_tpcdi.dim_trade: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_trade"} */

  
    
        create table demo_gold.dim_trade
      
      
      
      
      
      
      
      

      as
      select 
    md5(cast(concat(coalesce(cast(trade_id as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(t.effective_timestamp as string), '_dbt_utils_surrogate_key_null_')) as string)) sk_trade_id,
    trade_id,
    trade_status status,
    transaction_type,
    trade_type type,
    executor_name executed_by,
    t.effective_timestamp,
    t.end_timestamp,
    t.IS_CURRENT
from
    demo_silver.trades_history t
  
24/06/20 17:18:38 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:20:34  SQL status: OK in 116.0 seconds
17:20:34  Timing info for model.tbd_tpcdi.dim_trade (execute): 17:18:38.394247 => 17:20:34.560681
17:20:34  On model.tbd_tpcdi.dim_trade: ROLLBACK
17:20:34  Spark adapter: NotImplemented: rollback
17:20:34  On model.tbd_tpcdi.dim_trade: Close
17:20:34  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c45b130>]}
17:20:34  34 of 45 OK created sql table model demo_gold.dim_trade ........................ [OK in 116.20s]
17:20:34  Finished running node model.tbd_tpcdi.dim_trade
17:20:34  Began running node model.tbd_tpcdi.trades
17:20:34  35 of 45 START sql table model demo_silver.trades .............................. [RUN]
17:20:34  Re-using an available connection from the pool (formerly model.tbd_tpcdi.dim_trade, now model.tbd_tpcdi.trades)
17:20:34  Began compiling node model.tbd_tpcdi.trades
17:20:34  Writing injected SQL for node "model.tbd_tpcdi.trades"
17:20:34  Timing info for model.tbd_tpcdi.trades (compile): 17:20:34.587821 => 17:20:34.598552
17:20:34  Began executing node model.tbd_tpcdi.trades
17:20:34  Using spark connection "model.tbd_tpcdi.trades"
17:20:34  On model.tbd_tpcdi.trades: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.trades"} */
drop table if exists demo_silver.trades
17:20:34  Opening a new connection, currently in state closed
17:20:34  SQL status: OK in 0.0 seconds
17:20:34  Writing runtime sql for node "model.tbd_tpcdi.trades"
17:20:34  Spark adapter: NotImplemented: add_begin_query
17:20:34  Using spark connection "model.tbd_tpcdi.trades"
17:20:34  On model.tbd_tpcdi.trades: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.trades"} */

  
    
        create table demo_silver.trades
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select distinct
        trade_id,
        account_id,
        trade_status,
        trade_type,
        transaction_type,
        symbol,
        executor_name,
        quantity,
        bid_price,
        trade_price,
        fee,
        commission,
        tax,
        min(effective_timestamp) over (partition by trade_id) create_timestamp,
        max(effective_timestamp) over (partition by trade_id) close_timestamp
    from
        demo_silver.trades_history
    order by trade_id, create_timestamp
)
select *
from s1
  
24/06/20 17:20:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:22:42  SQL status: OK in 128.0 seconds
17:22:42  Timing info for model.tbd_tpcdi.trades (execute): 17:20:34.601851 => 17:22:42.732789
17:22:42  On model.tbd_tpcdi.trades: ROLLBACK
17:22:42  Spark adapter: NotImplemented: rollback
17:22:42  On model.tbd_tpcdi.trades: Close
17:22:42  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d631d30>]}
17:22:42  35 of 45 OK created sql table model demo_silver.trades ......................... [OK in 128.18s]
17:22:42  Finished running node model.tbd_tpcdi.trades
17:22:42  Began running node model.tbd_tpcdi.wrk_company_financials
17:22:42  Re-using an available connection from the pool (formerly model.tbd_tpcdi.trades, now model.tbd_tpcdi.wrk_company_financials)
17:22:42  Began compiling node model.tbd_tpcdi.wrk_company_financials
17:22:42  Writing injected SQL for node "model.tbd_tpcdi.wrk_company_financials"
17:22:42  Timing info for model.tbd_tpcdi.wrk_company_financials (compile): 17:22:42.782050 => 17:22:42.793295
17:22:42  Finished running node model.tbd_tpcdi.wrk_company_financials
17:22:42  Began running node model.tbd_tpcdi.dim_security
17:22:42  36 of 45 START sql table model demo_gold.dim_security .......................... [RUN]
17:22:42  Re-using an available connection from the pool (formerly model.tbd_tpcdi.wrk_company_financials, now model.tbd_tpcdi.dim_security)
17:22:42  Began compiling node model.tbd_tpcdi.dim_security
17:22:42  Writing injected SQL for node "model.tbd_tpcdi.dim_security"
17:22:42  Timing info for model.tbd_tpcdi.dim_security (compile): 17:22:42.805397 => 17:22:42.820632
17:22:42  Began executing node model.tbd_tpcdi.dim_security
17:22:42  Using spark connection "model.tbd_tpcdi.dim_security"
17:22:42  On model.tbd_tpcdi.dim_security: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_security"} */
drop table if exists demo_gold.dim_security
17:22:42  Opening a new connection, currently in state closed
17:22:43  SQL status: OK in 0.0 seconds
17:22:43  Writing runtime sql for node "model.tbd_tpcdi.dim_security"
17:22:43  Spark adapter: NotImplemented: add_begin_query
17:22:43  Using spark connection "model.tbd_tpcdi.dim_security"
17:22:43  On model.tbd_tpcdi.dim_security: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_security"} */

  
    
        create table demo_gold.dim_security
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select
        symbol,
        issue_type issue,
        s.status,
        s.name,
        exchange_id,
        sk_company_id,
        shares_outstanding,
        first_trade_date,
        first_exchange_date,
        dividend,
        s.effective_timestamp,
        s.end_timestamp,
        s.IS_CURRENT
    from
        demo_silver.securities s
    join
        demo_gold.dim_company c
    on 
        s.company_id = c.company_id
    and
        s.effective_timestamp between c.effective_timestamp and c.end_timestamp
)
select
    md5(cast(concat(coalesce(cast(symbol as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(effective_timestamp as string), '_dbt_utils_surrogate_key_null_')) as string)) sk_security_id,
    *
from
    s1
  
24/06/20 17:22:43 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:22:52  SQL status: OK in 9.0 seconds
17:22:52  Timing info for model.tbd_tpcdi.dim_security (execute): 17:22:42.824085 => 17:22:52.308186
17:22:52  On model.tbd_tpcdi.dim_security: ROLLBACK
17:22:52  Spark adapter: NotImplemented: rollback
17:22:52  On model.tbd_tpcdi.dim_security: Close
17:22:52  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d6315b0>]}
17:22:52  36 of 45 OK created sql table model demo_gold.dim_security ..................... [OK in 9.52s]
17:22:52  Finished running node model.tbd_tpcdi.dim_security
17:22:52  Began running node model.tbd_tpcdi.watches_history
17:22:52  37 of 45 START sql table model demo_silver.watches_history ..................... [RUN]
17:22:52  Re-using an available connection from the pool (formerly model.tbd_tpcdi.dim_security, now model.tbd_tpcdi.watches_history)
17:22:52  Began compiling node model.tbd_tpcdi.watches_history
17:22:52  Writing injected SQL for node "model.tbd_tpcdi.watches_history"
17:22:52  Timing info for model.tbd_tpcdi.watches_history (compile): 17:22:52.336000 => 17:22:52.348199
17:22:52  Began executing node model.tbd_tpcdi.watches_history
17:22:52  Using spark connection "model.tbd_tpcdi.watches_history"
17:22:52  On model.tbd_tpcdi.watches_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.watches_history"} */
drop table if exists demo_silver.watches_history
17:22:52  Opening a new connection, currently in state closed
17:22:52  SQL status: OK in 0.0 seconds
17:22:52  Writing runtime sql for node "model.tbd_tpcdi.watches_history"
17:22:52  Spark adapter: NotImplemented: add_begin_query
17:22:52  Using spark connection "model.tbd_tpcdi.watches_history"
17:22:52  On model.tbd_tpcdi.watches_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.watches_history"} */

  
    
        create table demo_silver.watches_history
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select
        w_c_id customer_id,
        w_s_symb symbol,
        w_dts watch_timestamp,
        case w_action
        when 'ACTV' then 'Activate'
        when 'CNCL' then 'Cancelled'
        else null end action_type
    from
        demo_bronze.brokerage_watch_history
)
select 
    s1.*,
    company_id,
    company_name,
    exchange_id,
    status security_status
from 
    s1
join
    demo_silver.securities s
using (symbol)
  
24/06/20 17:22:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:28:39  SQL status: OK in 347.0 seconds
17:28:39  Timing info for model.tbd_tpcdi.watches_history (execute): 17:22:52.351472 => 17:28:39.106742
17:28:39  On model.tbd_tpcdi.watches_history: ROLLBACK
17:28:39  Spark adapter: NotImplemented: rollback
17:28:39  On model.tbd_tpcdi.watches_history: Close
17:28:39  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c45b130>]}
17:28:39  37 of 45 OK created sql table model demo_silver.watches_history ................ [OK in 346.79s]
17:28:39  Finished running node model.tbd_tpcdi.watches_history
17:28:39  Began running node model.tbd_tpcdi.dim_account
17:28:39  38 of 45 START sql table model demo_gold.dim_account ........................... [RUN]
17:28:39  Re-using an available connection from the pool (formerly model.tbd_tpcdi.watches_history, now model.tbd_tpcdi.dim_account)
17:28:39  Began compiling node model.tbd_tpcdi.dim_account
17:28:39  Writing injected SQL for node "model.tbd_tpcdi.dim_account"
17:28:39  Timing info for model.tbd_tpcdi.dim_account (compile): 17:28:39.137193 => 17:28:39.153128
17:28:39  Began executing node model.tbd_tpcdi.dim_account
17:28:39  Using spark connection "model.tbd_tpcdi.dim_account"
17:28:39  On model.tbd_tpcdi.dim_account: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_account"} */
drop table if exists demo_gold.dim_account
17:28:39  Opening a new connection, currently in state closed
17:28:39  SQL status: OK in 0.0 seconds
17:28:39  Writing runtime sql for node "model.tbd_tpcdi.dim_account"
17:28:39  Spark adapter: NotImplemented: add_begin_query
17:28:39  Using spark connection "model.tbd_tpcdi.dim_account"
17:28:39  On model.tbd_tpcdi.dim_account: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.dim_account"} */

  
    
        create table demo_gold.dim_account
      
      
      
      
      
      
      
      

      as
      SELECT
    md5(cast(concat(coalesce(cast(account_id as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(a.effective_timestamp as string), '_dbt_utils_surrogate_key_null_')) as string)) sk_account_id,
    a.account_id,
    sk_broker_id,
    sk_customer_id,
    a.status,
    account_desc,
    tax_status,
    a.effective_timestamp,
    a.end_timestamp,
    a.is_current
from
    demo_silver.accounts a
join
    demo_gold.dim_customer c
on a.customer_id = c.customer_id
and a.effective_timestamp between c.effective_timestamp and c.end_timestamp
join
    demo_gold.dim_broker b 
using (broker_id)
  
24/06/20 17:28:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:29:05  SQL status: OK in 26.0 seconds
17:29:05  Timing info for model.tbd_tpcdi.dim_account (execute): 17:28:39.158051 => 17:29:05.756462
17:29:05  On model.tbd_tpcdi.dim_account: ROLLBACK
17:29:05  Spark adapter: NotImplemented: rollback
17:29:05  On model.tbd_tpcdi.dim_account: Close
17:29:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c47cf40>]}
17:29:05  38 of 45 OK created sql table model demo_gold.dim_account ...................... [OK in 26.63s]
17:29:05  Finished running node model.tbd_tpcdi.dim_account
17:29:05  Began running node model.tbd_tpcdi.holdings_history
17:29:05  39 of 45 START sql table model demo_silver.holdings_history .................... [RUN]
17:29:05  Re-using an available connection from the pool (formerly model.tbd_tpcdi.dim_account, now model.tbd_tpcdi.holdings_history)
17:29:05  Began compiling node model.tbd_tpcdi.holdings_history
17:29:05  Writing injected SQL for node "model.tbd_tpcdi.holdings_history"
17:29:05  Timing info for model.tbd_tpcdi.holdings_history (compile): 17:29:05.771655 => 17:29:05.781358
17:29:05  Began executing node model.tbd_tpcdi.holdings_history
17:29:05  Using spark connection "model.tbd_tpcdi.holdings_history"
17:29:05  On model.tbd_tpcdi.holdings_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.holdings_history"} */
drop table if exists demo_silver.holdings_history
17:29:05  Opening a new connection, currently in state closed
17:29:06  SQL status: OK in 0.0 seconds
17:29:06  Writing runtime sql for node "model.tbd_tpcdi.holdings_history"
17:29:06  Spark adapter: NotImplemented: add_begin_query
17:29:06  Using spark connection "model.tbd_tpcdi.holdings_history"
17:29:06  On model.tbd_tpcdi.holdings_history: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.holdings_history"} */

  
    
        create table demo_silver.holdings_history
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select
        HH_T_ID trade_id,
        HH_H_T_ID previous_trade_id,
        hh_before_qty previous_quantity,
        hh_after_qty quantity
    from demo_bronze.brokerage_holding_history
)
select s1.*,
       ct.account_id account_id,
       ct.symbol symbol,
       ct.create_timestamp,
       ct.close_timestamp,
       ct.trade_price,
       ct.bid_price,
       ct.fee,
       ct.commission
from s1
join demo_silver.trades ct
using (trade_id)
  
24/06/20 17:29:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:31:54  SQL status: OK in 169.0 seconds
17:31:54  Timing info for model.tbd_tpcdi.holdings_history (execute): 17:29:05.783840 => 17:31:54.690722
17:31:54  On model.tbd_tpcdi.holdings_history: ROLLBACK
17:31:54  Spark adapter: NotImplemented: rollback
17:31:54  On model.tbd_tpcdi.holdings_history: Close
17:31:54  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c45e640>]}
17:31:54  39 of 45 OK created sql table model demo_silver.holdings_history ............... [OK in 168.93s]
17:31:54  Finished running node model.tbd_tpcdi.holdings_history
17:31:54  Began running node model.tbd_tpcdi.watches
17:31:54  40 of 45 START sql table model demo_silver.watches ............................. [RUN]
17:31:54  Re-using an available connection from the pool (formerly model.tbd_tpcdi.holdings_history, now model.tbd_tpcdi.watches)
17:31:54  Began compiling node model.tbd_tpcdi.watches
17:31:54  Writing injected SQL for node "model.tbd_tpcdi.watches"
17:31:54  Timing info for model.tbd_tpcdi.watches (compile): 17:31:54.708063 => 17:31:54.720570
17:31:54  Began executing node model.tbd_tpcdi.watches
17:31:54  Using spark connection "model.tbd_tpcdi.watches"
17:31:54  On model.tbd_tpcdi.watches: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.watches"} */
drop table if exists demo_silver.watches
17:31:54  Opening a new connection, currently in state closed
17:31:54  SQL status: OK in 0.0 seconds
17:31:54  Writing runtime sql for node "model.tbd_tpcdi.watches"
17:31:54  Spark adapter: NotImplemented: add_begin_query
17:31:54  Using spark connection "model.tbd_tpcdi.watches"
17:31:54  On model.tbd_tpcdi.watches: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.watches"} */

  
    
        create table demo_silver.watches
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select
        customer_id,
        symbol,
        watch_timestamp,
        action_type,
        company_id,
        company_name,
        exchange_id,
        security_status,
        case action_type
            when 'Activate' then watch_timestamp 
            else null 
        end placed_timestamp,
        case action_type
            when 'Cancelled' then watch_timestamp 
            else null 
        end removed_timestamp
    from
        demo_silver.watches_history
),
s2 as (
    select
        customer_id,
        symbol,
        company_id,
        company_name,
        exchange_id,
        security_status,
        min(placed_timestamp) placed_timestamp, 
        max(removed_timestamp) removed_timestamp
    from s1
    group by customer_id, symbol, company_id, company_name, exchange_id, security_status
)
select 
    *,
    case
        when removed_timestamp is null then 'Active'
        else 'Inactive'
    end watch_status
from s2
  
24/06/20 17:31:55 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:36:42  SQL status: OK in 288.0 seconds
17:36:42  Timing info for model.tbd_tpcdi.watches (execute): 17:31:54.723894 => 17:36:42.937060
17:36:42  On model.tbd_tpcdi.watches: ROLLBACK
17:36:42  Spark adapter: NotImplemented: rollback
17:36:42  On model.tbd_tpcdi.watches: Close
17:36:42  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c41be80>]}
17:36:42  40 of 45 OK created sql table model demo_silver.watches ........................ [OK in 288.25s]
17:36:42  Finished running node model.tbd_tpcdi.watches
17:36:42  Began running node model.tbd_tpcdi.fact_cash_transactions
17:36:42  41 of 45 START sql table model demo_gold.fact_cash_transactions ................ [RUN]
17:36:42  Re-using an available connection from the pool (formerly model.tbd_tpcdi.watches, now model.tbd_tpcdi.fact_cash_transactions)
17:36:42  Began compiling node model.tbd_tpcdi.fact_cash_transactions
17:36:42  Writing injected SQL for node "model.tbd_tpcdi.fact_cash_transactions"
17:36:42  Timing info for model.tbd_tpcdi.fact_cash_transactions (compile): 17:36:42.975882 => 17:36:42.987763
17:36:42  Began executing node model.tbd_tpcdi.fact_cash_transactions
17:36:43  Using spark connection "model.tbd_tpcdi.fact_cash_transactions"
17:36:43  On model.tbd_tpcdi.fact_cash_transactions: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_cash_transactions"} */
drop table if exists demo_gold.fact_cash_transactions
17:36:43  Opening a new connection, currently in state closed
17:36:43  SQL status: OK in 0.0 seconds
17:36:43  Writing runtime sql for node "model.tbd_tpcdi.fact_cash_transactions"
17:36:43  Spark adapter: NotImplemented: add_begin_query
17:36:43  Using spark connection "model.tbd_tpcdi.fact_cash_transactions"
17:36:43  On model.tbd_tpcdi.fact_cash_transactions: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_cash_transactions"} */

  
    
        create table demo_gold.fact_cash_transactions
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select
        *,
        to_date(transaction_timestamp) sk_transaction_date
    from
        demo_silver.cash_transactions
)
select
    sk_customer_id,
    sk_account_id,
    sk_transaction_date,
    transaction_timestamp,
    amount,
    description
from
    s1
join
    demo_gold.dim_account a
on
    s1.account_id = a.account_id
and
    s1.transaction_timestamp between a.effective_timestamp and a.end_timestamp
  
24/06/20 17:36:43 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:40:32  SQL status: OK in 229.0 seconds
17:40:32  Timing info for model.tbd_tpcdi.fact_cash_transactions (execute): 17:36:42.991103 => 17:40:32.434711
17:40:32  On model.tbd_tpcdi.fact_cash_transactions: ROLLBACK
17:40:32  Spark adapter: NotImplemented: rollback
17:40:32  On model.tbd_tpcdi.fact_cash_transactions: Close
17:40:32  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4bd760>]}
17:40:32  41 of 45 OK created sql table model demo_gold.fact_cash_transactions ........... [OK in 229.48s]
17:40:32  Finished running node model.tbd_tpcdi.fact_cash_transactions
17:40:32  Began running node model.tbd_tpcdi.fact_trade
17:40:32  42 of 45 START sql table model demo_gold.fact_trade ............................ [RUN]
17:40:32  Re-using an available connection from the pool (formerly model.tbd_tpcdi.fact_cash_transactions, now model.tbd_tpcdi.fact_trade)
17:40:32  Began compiling node model.tbd_tpcdi.fact_trade
17:40:32  Writing injected SQL for node "model.tbd_tpcdi.fact_trade"
17:40:32  Timing info for model.tbd_tpcdi.fact_trade (compile): 17:40:32.483258 => 17:40:32.500824
17:40:32  Began executing node model.tbd_tpcdi.fact_trade
17:40:32  Using spark connection "model.tbd_tpcdi.fact_trade"
17:40:32  On model.tbd_tpcdi.fact_trade: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_trade"} */
drop table if exists demo_gold.fact_trade
17:40:32  Opening a new connection, currently in state closed
17:40:32  SQL status: OK in 0.0 seconds
17:40:32  Writing runtime sql for node "model.tbd_tpcdi.fact_trade"
17:40:32  Spark adapter: NotImplemented: add_begin_query
17:40:32  Using spark connection "model.tbd_tpcdi.fact_trade"
17:40:32  On model.tbd_tpcdi.fact_trade: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_trade"} */

  
    
        create table demo_gold.fact_trade
      
      
      
      
      
      
      
      

      as
      select
    sk_trade_id,
    sk_broker_id,
    sk_customer_id,
    sk_account_id,
    sk_security_id,
    to_date(create_timestamp) sk_create_date,
    create_timestamp,
    to_date(close_timestamp) sk_close_date,
    close_timestamp,
    executed_by,
    quantity,
    bid_price,
    trade_price,
    fee,
    commission,
    tax
from demo_silver.trades t
join demo_gold.dim_trade dt
on t.trade_id = dt.trade_id
and t.create_timestamp between dt.effective_timestamp and dt.end_timestamp
join
    demo_gold.dim_account a
on 
    t.account_id = a.account_id
and
    t.create_timestamp between a.effective_timestamp and a.end_timestamp
join
    demo_gold.dim_security s
on
    t.symbol = s.symbol
and
    t.create_timestamp between s.effective_timestamp and s.end_timestamp
  
24/06/20 17:40:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
17:49:47  SQL status: OK in 555.0 seconds
17:49:47  Timing info for model.tbd_tpcdi.fact_trade (execute): 17:40:32.506882 => 17:49:47.786738
17:49:47  On model.tbd_tpcdi.fact_trade: ROLLBACK
17:49:47  Spark adapter: NotImplemented: rollback
17:49:47  On model.tbd_tpcdi.fact_trade: Close
17:49:47  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d64b3d0>]}
17:49:47  42 of 45 OK created sql table model demo_gold.fact_trade ....................... [OK in 555.32s]
17:49:47  Finished running node model.tbd_tpcdi.fact_trade
17:49:47  Began running node model.tbd_tpcdi.fact_holdings
17:49:47  43 of 45 START sql table model demo_gold.fact_holdings ......................... [RUN]
17:49:47  Re-using an available connection from the pool (formerly model.tbd_tpcdi.fact_trade, now model.tbd_tpcdi.fact_holdings)
17:49:47  Began compiling node model.tbd_tpcdi.fact_holdings
17:49:47  Writing injected SQL for node "model.tbd_tpcdi.fact_holdings"
17:49:47  Timing info for model.tbd_tpcdi.fact_holdings (compile): 17:49:47.805805 => 17:49:47.820811
17:49:47  Began executing node model.tbd_tpcdi.fact_holdings
17:49:47  Using spark connection "model.tbd_tpcdi.fact_holdings"
17:49:47  On model.tbd_tpcdi.fact_holdings: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_holdings"} */
drop table if exists demo_gold.fact_holdings
17:49:47  Opening a new connection, currently in state closed
17:49:48  SQL status: OK in 0.0 seconds
17:49:48  Writing runtime sql for node "model.tbd_tpcdi.fact_holdings"
17:49:48  Spark adapter: NotImplemented: add_begin_query
17:49:48  Using spark connection "model.tbd_tpcdi.fact_holdings"
17:49:48  On model.tbd_tpcdi.fact_holdings: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_holdings"} */

  
    
        create table demo_gold.fact_holdings
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select 
        *
    from demo_silver.holdings_history
)
select
    ct.sk_trade_id sk_current_trade_id,
    pt.sk_trade_id,
    sk_customer_id,
    sk_account_id,
    sk_security_id,
    to_date(create_timestamp) sk_trade_date,
    create_timestamp trade_timestamp,
    trade_price current_price,
    quantity current_holding,
    bid_price current_bid_price,
    fee current_fee,
    commission current_commission
from s1
join demo_gold.dim_trade ct
using (trade_id)
join demo_gold.dim_trade pt 
on s1.previous_trade_id = pt.trade_id
join demo_gold.dim_account a 
on s1.account_id = a.account_id 
and s1.create_timestamp between a.effective_timestamp and a.end_timestamp
join demo_gold.dim_security s 
on s1.symbol = s.symbol
  
24/06/20 17:49:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
18:05:06  SQL status: OK in 918.0 seconds
18:05:06  Timing info for model.tbd_tpcdi.fact_holdings (execute): 17:49:47.824633 => 18:05:06.217905
18:05:06  On model.tbd_tpcdi.fact_holdings: ROLLBACK
18:05:06  Spark adapter: NotImplemented: rollback
18:05:06  On model.tbd_tpcdi.fact_holdings: Close
18:05:06  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c4cd2b0>]}
18:05:06  43 of 45 OK created sql table model demo_gold.fact_holdings .................... [OK in 918.44s]
18:05:06  Finished running node model.tbd_tpcdi.fact_holdings
18:05:06  Began running node model.tbd_tpcdi.fact_watches
18:05:06  44 of 45 START sql table model demo_gold.fact_watches .......................... [RUN]
18:05:06  Re-using an available connection from the pool (formerly model.tbd_tpcdi.fact_holdings, now model.tbd_tpcdi.fact_watches)
18:05:06  Began compiling node model.tbd_tpcdi.fact_watches
18:05:06  Writing injected SQL for node "model.tbd_tpcdi.fact_watches"
18:05:06  Timing info for model.tbd_tpcdi.fact_watches (compile): 18:05:06.272078 => 18:05:06.283454
18:05:06  Began executing node model.tbd_tpcdi.fact_watches
18:05:06  Using spark connection "model.tbd_tpcdi.fact_watches"
18:05:06  On model.tbd_tpcdi.fact_watches: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_watches"} */
drop table if exists demo_gold.fact_watches
18:05:06  Opening a new connection, currently in state closed
18:05:06  SQL status: OK in 0.0 seconds
18:05:06  Writing runtime sql for node "model.tbd_tpcdi.fact_watches"
18:05:06  Spark adapter: NotImplemented: add_begin_query
18:05:06  Using spark connection "model.tbd_tpcdi.fact_watches"
18:05:06  On model.tbd_tpcdi.fact_watches: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_watches"} */

  
    
        create table demo_gold.fact_watches
      
      
      
      
      
      
      
      

      as
      select
    sk_customer_id,
    sk_security_id,
    to_date(placed_timestamp) sk_date_placed,
    to_date(removed_timestamp) sk_date_removed,
    1 as watch_cnt
from 
    demo_silver.watches w
join
    demo_gold.dim_customer c
ON
    w.customer_id = c.customer_id
and
    placed_timestamp between c.effective_timestamp and c.end_timestamp
join
    demo_gold.dim_security s
ON
    w.symbol = s.symbol
and
    placed_timestamp between s.effective_timestamp and s.end_timestamp
  
24/06/20 18:05:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
18:09:10  SQL status: OK in 244.0 seconds
18:09:10  Timing info for model.tbd_tpcdi.fact_watches (execute): 18:05:06.285955 => 18:09:10.217552
18:09:10  On model.tbd_tpcdi.fact_watches: ROLLBACK
18:09:10  Spark adapter: NotImplemented: rollback
18:09:10  On model.tbd_tpcdi.fact_watches: Close
18:09:10  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7d64b3d0>]}
18:09:10  44 of 45 OK created sql table model demo_gold.fact_watches ..................... [OK in 243.96s]
18:09:10  Finished running node model.tbd_tpcdi.fact_watches
18:09:10  Began running node model.tbd_tpcdi.fact_cash_balances
18:09:10  45 of 45 START sql table model demo_gold.fact_cash_balances .................... [RUN]
18:09:10  Re-using an available connection from the pool (formerly model.tbd_tpcdi.fact_watches, now model.tbd_tpcdi.fact_cash_balances)
18:09:10  Began compiling node model.tbd_tpcdi.fact_cash_balances
18:09:10  Writing injected SQL for node "model.tbd_tpcdi.fact_cash_balances"
18:09:10  Timing info for model.tbd_tpcdi.fact_cash_balances (compile): 18:09:10.233280 => 18:09:10.242204
18:09:10  Began executing node model.tbd_tpcdi.fact_cash_balances
18:09:10  Using spark connection "model.tbd_tpcdi.fact_cash_balances"
18:09:10  On model.tbd_tpcdi.fact_cash_balances: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_cash_balances"} */
drop table if exists demo_gold.fact_cash_balances
18:09:10  Opening a new connection, currently in state closed
18:09:10  SQL status: OK in 0.0 seconds
18:09:10  Writing runtime sql for node "model.tbd_tpcdi.fact_cash_balances"
18:09:10  Spark adapter: NotImplemented: add_begin_query
18:09:10  Using spark connection "model.tbd_tpcdi.fact_cash_balances"
18:09:10  On model.tbd_tpcdi.fact_cash_balances: /* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.fact_cash_balances"} */

  
    
        create table demo_gold.fact_cash_balances
      
      
      
      
      
      
      
      

      as
      with s1 as (
    select *
    from demo_gold.fact_cash_transactions
)
select 
    sk_customer_id,
    sk_account_id,
    sk_transaction_date,
    sum(amount) amount,
    description
from s1
group by sk_customer_id, sk_account_id, sk_transaction_date, description
order by sk_transaction_date, sk_customer_id, sk_account_id
  
24/06/20 18:09:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
18:12:42  SQL status: OK in 212.0 seconds
18:12:42  Timing info for model.tbd_tpcdi.fact_cash_balances (execute): 18:09:10.244684 => 18:12:42.563879
18:12:42  On model.tbd_tpcdi.fact_cash_balances: ROLLBACK
18:12:42  Spark adapter: NotImplemented: rollback
18:12:42  On model.tbd_tpcdi.fact_cash_balances: Close
18:12:42  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3413dded-052d-4a81-adb5-ad0641262d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7c41d970>]}
18:12:42  45 of 45 OK created sql table model demo_gold.fact_cash_balances ............... [OK in 212.36s]
18:12:42  Finished running node model.tbd_tpcdi.fact_cash_balances
18:12:42  On master: ROLLBACK
18:12:42  Opening a new connection, currently in state init
18:12:42  Spark adapter: NotImplemented: rollback
18:12:42  Spark adapter: NotImplemented: add_begin_query
18:12:42  Spark adapter: NotImplemented: commit
18:12:42  On master: ROLLBACK
18:12:42  Spark adapter: NotImplemented: rollback
18:12:42  On master: Close
18:12:42  Connection 'master' was properly closed.
18:12:42  Connection 'list_schemas' was properly closed.
18:12:42  Connection 'model.tbd_tpcdi.fact_cash_balances' was properly closed.
18:12:42  
18:12:42  Finished running 45 table models in 1 hours 13 minutes and 30.82 seconds (4410.82s).
18:12:42  Command end result
18:12:42  
18:12:42  Completed with 2 errors and 0 warnings:
18:12:42  
18:12:42    
Possibly unquoted identifier brokerage_cash_transaction-checkpoint detected. Please consider quoting it with back-quotes as `brokerage_cash_transaction-checkpoint`(line 2, pos 59)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.brokerage_cash_transaction-checkpoint"} */
drop table if exists demo_bronze.brokerage_cash_transaction-checkpoint
-----------------------------------------------------------^^^

18:12:42  
18:12:42    
Possibly unquoted identifier daily_market-checkpoint detected. Please consider quoting it with back-quotes as `daily_market-checkpoint`(line 2, pos 45)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.13", "profile_name": "tbd_tpcdi", "target_name": "dev", "node_id": "model.tbd_tpcdi.daily_market-checkpoint"} */
drop table if exists demo_silver.daily_market-checkpoint
---------------------------------------------^^^

18:12:42  
18:12:42  Done. PASS=43 WARN=0 ERROR=2 SKIP=0 TOTAL=45
18:12:42  Resource report: {"command_name": "run", "command_wall_clock_time": 4411.9907, "process_user_time": 8.671883, "process_kernel_time": 0.779694, "process_mem_max_rss": "130156", "process_out_blocks": "6344", "command_success": false, "process_in_blocks": "0"}
18:12:42  Command `dbt run` failed at 18:12:42.783782 after 4411.99 seconds
18:12:42  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd86d79670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7f394d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd7f4acca0>]}
18:12:42  Flushing usage events
ERROR:asyncio:Task exception was never retrieved
future: <Task finished name='Task-188' coro=<ScriptMagics.shebang.<locals>._handle_stream() done, defined at /usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py:211> exception=ValueError('Separator is not found, and chunk exceed the limit')>
Traceback (most recent call last):
  File "/usr/lib/python3.8/asyncio/streams.py", line 540, in readline
    line = await self.readuntil(sep)
  File "/usr/lib/python3.8/asyncio/streams.py", line 618, in readuntil
    raise exceptions.LimitOverrunError(
asyncio.exceptions.LimitOverrunError: Separator is not found, and chunk exceed the limit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py", line 213, in _handle_stream
    line = (await stream.readline()).decode("utf8", errors="replace")
  File "/usr/lib/python3.8/asyncio/streams.py", line 549, in readline
    raise ValueError(e.args[0])
ValueError: Separator is not found, and chunk exceed the limit
---------------------------------------------------------------------------
CalledProcessError                        Traceback (most recent call last)
Cell In[95], line 1
----> 1 get_ipython().run_cell_magic('bash', '', 'cd $REPO_ROOT\ndbt deps\ndbt run -d\n')

File /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:2478, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)
   2476 with self.builtin_trap:
   2477     args = (magic_arg_s, cell)
-> 2478     result = fn(*args, **kwargs)
   2480 # The code below prevents the output from being displayed
   2481 # when using magics with decodator @output_can_be_silenced
   2482 # when the last Python token in the expression is a ';'.
   2483 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):

File /usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py:153, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)
    151 else:
    152     line = script
--> 153 return self.shebang(line, cell)

File /usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py:305, in ScriptMagics.shebang(self, line, cell)
    300 if args.raise_error and p.returncode != 0:
    301     # If we get here and p.returncode is still None, we must have
    302     # killed it but not yet seen its return code. We don't wait for it,
    303     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL
    304     rc = p.returncode or -9
--> 305     raise CalledProcessError(rc, cell)

CalledProcessError: Command 'b'cd $REPO_ROOT\ndbt deps\ndbt run -d\n'' returned non-zero exit status 1.
